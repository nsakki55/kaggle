{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap of main ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代表的な分類器の概要\n",
    "## Linear model\n",
    "Logistic Regression  \n",
    "SVM\n",
    "最もシンプルな分類は直線でに２種類のデータを分割すること  \n",
    "分類器それぞれに独自のコスト関数が決められている。  \n",
    "非線形分離ができないという問題点を指摘  \n",
    "## Tree-based\n",
    "### 決定木、ランダムフォレスト、勾配ブースティング\n",
    "テーブルデータを用いるときは最も良い選択といえる場合が多い。データを様々な基準で分割することで、クラスわけを行なう。  \n",
    "そのため、たくさんの決定木を考えることができるため、選択肢が増える。  \n",
    "欠点は、決定木はデータの線形関係を捉えることができないため、線形分類可能なデータに対しての性能が悪くなる場合がある。\n",
    "## k-NN-based\n",
    "### k-nearest neighbor\n",
    "新しいデータ点のラベルを予測する際に、新しいデータに最も近いデータのラベルを採用するという、シンプルなアルゴリズム。  \n",
    "近いオブジェクトが最も近いラベルを持っているだろうという、シンプルな考え方\n",
    "## Neural Networks\n",
    "特殊な分類機として取り上げられている。\n",
    "\n",
    "## No Free Lunch Theorem\n",
    "各アルゴリズムは、それぞれの分類タスクに対しては機能するが、他の分類タスクには機能しないというもの。  \n",
    "各分類機は、データの想定に依存しているため。\n",
    "\n",
    "## 結論\n",
    "・silver bulletなアルゴリズムはない  \n",
    "・線形モデルは、特徴量空間を２つに分割する  \n",
    "・決定木は、特徴量空間をボックスに分割する  \n",
    "・k-NNは最も近い点に強く依存する  \n",
    "・ニューラルネットワークは非線形分類モデル  \n",
    "大抵の場合は勾配ブースティングかニューラルネットワークが機能するが、線形分離可能なモデルの場合には別のモデルも使うべき"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature preprocessing and generation with respect to models\n",
    "# 特徴量の事前処理\n",
    "## overview\n",
    "特徴量には、categorical特徴量、numeric特徴量、文字列特徴量、時系列特徴量があり、それぞれに必要な処理がある。  \n",
    "データ前処理は、新しい特徴量を作成するのに必要となる。  \n",
    "・前処理は必ず必要となる  \n",
    "・特徴量生成は強力な技となる  \n",
    "・前処理と特徴量生成は使うモデルに依存する。\n",
    "\n",
    "## Numeric Features\n",
    "### tree-based model\n",
    "決定木はデータのスケールに影響を受けない\n",
    "### non-tree-based-model\n",
    "k-NNやLinear model, Neural Networkは特徴量のスケールに大きく影響を受ける。特にk-NNではサイズの大きい特徴量が重要な特徴量とされてしまう。  \n",
    "すべてのnumeric特徴量に対して、前処理をしなければならない。\n",
    "### MinMaxScaler\n",
    "最大値と最小値を揃えるスケーリング処理  \n",
    "ヒストグラムの形は処理を加えた前後で変化しない。  \n",
    "### StandardScaler\n",
    "標準化の説明。\n",
    "### 外れ値の処理\n",
    "線形分離では、外れ値に多き日引っ張られて、決定境界が変わってしまう。  \n",
    "ヒストグラムで、データの分布を見た時に外れ値があるか確認できる。\n",
    "<img src=\"images/numeric1.png\" width='400px'>\n",
    "\n",
    "### Rank Scaler\n",
    "外れ値への対応として有効な手段。特徴量の大きさをランキングした際のindicesを特徴呂とすることで、もとのスケールの違いを軽減させることができる。kNN,Linear model,Nreural Networkに有効な手段。  \n",
    "手作業で外れ値の処理が難しい際に有効な処理となる。  \n",
    "scipy.stats.rankdataで実装可能\n",
    "### log transform\n",
    "np.log(1+x)\n",
    "### 平方根をとる\n",
    "np.sqrt(x+2/3)\n",
    "大きなスケールの特徴量を平均の値に近づけることができる。0に近い値を分類しやすい大きさに変えることができる。\n",
    "## 特徴量生成\n",
    "prior knowledge,EDAを行なうことで、特徴量間の関係性を明らかにし、新しい特徴量の生成を行なう。ex) 面積、価格→単位面積あたりの価格。  \n",
    "特徴量同士の和、積、商を取ることで新しい特徴量を生成する。\n",
    "<img src=\"images/numeric2.png\" width='400px'>\n",
    "\n",
    "## Categorical and Oridinal Features\n",
    "categoical特徴量の中には、意図的にランク付けされているものがある。これらの特徴量をordinal Featuresと呼ぶ。ex) Ticket Class:1,2,3　Driver licence:A,B,C,D Education: kindergarden, school, undergraduate, bachelor, master それぞれには順序には意味がある。  \n",
    "label encodingとfrequency encodingは決定木に対して有効な場合が多い\n",
    "### Label Encoding\n",
    "1. Alphabetical (sorted) アルファベット順に数値をつける。記号の順番に意味がある時に有効  \n",
    "[S,C,Q] -> [2,1,3]    \n",
    "sklearnのsklearn.preprcessing.LabelEncoderで実装\n",
    "\n",
    "2. Order of appearence　特徴量が出現した順に番号をつけていく。各記号に大小関係がない場合は有効  \n",
    "[S,C,Q] -> [1,2,3]  \n",
    "Pandas.factorizeで実装可能 \n",
    "\n",
    "### Frequency encoding \n",
    "各特徴量の出現率でエンコードする方法。割合が当てはめられるので特殊.S,C,S,S,S,Q,S,S,S,C,S  \n",
    "[S,C,Q] -> [0.5,0.2,0.3]   \n",
    "pandasのmapで実装可能,titanic.embarked.map(encoding)\n",
    "\n",
    "### one-hot encoding\n",
    "定番のワンほっとエンコーディング。pandas.get_dummiesm skelarn.preprocessing,OneHot-Encoderで実装可能  \n",
    "決定木に対しては有効な手段とは言えない。ユニークな要素が多いcategorical特徴量ではスパースなデータとなり、メモリに乗り切らない可能性がある。  \n",
    "categorical特徴量や、文字列特徴量に有効な処理といえる。  \n",
    "非決定木分類に対してワンホットエンコーディングは有効な場合が多い\n",
    "\n",
    "## 特徴量生成\n",
    "２つ以上のcategorical,文字列特徴量のラベルを合わせて、分類を行なう。ex) pclass(1,2,3), sex(man,female)を合わせたpclass_sex(1_male,2_male...2_female,3_female)特徴量を作成し、ワンホットエンコーディングを行なう\n",
    "<img src=\"images/categorical1.png\" width='300px'>\n",
    "\n",
    "## Datetime and Coorinates 特徴量\n",
    "時系列データの処理の方法を概説  \n",
    "\n",
    "### Date and time\n",
    "1. Preriodicity  \n",
    "時間をweek,month,season,year,second,minute,hourなどの期間で分割をする  \n",
    "<img src=\"images/datetime1.png\" width='400px'>\n",
    "2. Time since  \n",
    "特定のイベントからどれほどの時間が経過したかを考える。  \n",
    "<img src=\"images/datetime2.png\" width='400px'>\n",
    "\n",
    "3. Difference between dates\n",
    "datetime_1 - datetime_2 のように２つの時系列データの差を取る。  \n",
    "<img src=\"images/datetime3.png\" width='400px'>\n",
    "\n",
    "###  coordinate(座標)\n",
    "1. interestion place from train/test data or addtional data\n",
    "座標上から、興味深い位置を特定する\n",
    "2. center of cluster\n",
    "クラスタリングの中心座標を取得し、距離を計算する\n",
    "3. aggregated statics\n",
    "周囲の場所から統計処理を行なう。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values\n",
    "欠損値は各データごとに値がことなる.\n",
    "Nan,-1,-999,-99など、異なる値を取ることがある。欠損値自身にも重要な意味がある場合がある  \n",
    "ex) -1の値を欠損値として認識する方法。ヒストグラムを取った時に、-1が外れ値として存在している場合は欠損している値を-1にしたと考えることができる。  \n",
    "ヒストグラムの一部が極端に突き出ている場合は、欠損値に平均値を入れたと考えることができる。この場合は、欠損値の存在が隠されてしまうので、慎重にデータの分布を確認する必要がある。\n",
    "<img src=\"images/missing1.png\" width='400px'>\n",
    "\n",
    "### Fillna approches\n",
    "1. -999, -1 ,etc  \n",
    "最も簡単な方法だが、Linear modelのパフォーマンスは悪くなる\n",
    "2. mean,median  \n",
    "Linear modelやNerural Networkに対して有効な処理。一方で、決定木に対しては、どの値が欠損値なのか判別ができなくなるので、性能が悪くなる\n",
    "3. reconstruct value\n",
    "<img src=\"images/missing2.png\" width='400px'>\n",
    "一般的に、特徴量生成を行なう前に欠損値補完を行なうことは避ける。  \n",
    "XGBoostはNaNデータを扱うことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Bag of words\n",
    "# 文字列データの扱い方\n",
    "textをベクトル化するには主な手法は２種類\n",
    "<img src=\"images/bagofword1.png\" width='400px'>\n",
    "\n",
    "Bag of wordsでは、各単語の出現回数をカウントする。  \n",
    "データ前処理では、モデルが各単語を比較できる形にする。  \n",
    "出現回数は、割合を用いる方が、比較が行いやすくなる。  \n",
    "実装はsklearn.feature_extraction.text.TfidfVectorizerで行える\n",
    "\n",
    "<img src=\"images/bagofword2.png\" width='400px'>  \n",
    "### TF + iDF\n",
    "<img src=\"images/bagofword3.png\" width='400px'>\n",
    "\n",
    "### N-grams  \n",
    "単語を複数個をセットにして考える。  \n",
    "sklearn.feature_extraction.text.CountVectorizerで実装\n",
    "<img src=\"images/bagofword4.png\" width='400px'>\n",
    "### テキスト前処理\n",
    "#### lowercase\n",
    "すべての文字を小文字にすることで、大文字になっていないために別の文字として認識されないようにする。sklearnではデフォルトで、小文字にする処理が備わっている。\n",
    "<img src=\"images/bagofword5.png\" width='400px'>\n",
    "#### lemmatization stemming\n",
    "<img src=\"images/bagofword6.png\" width='400px'>\n",
    "\n",
    "Bag of WordsのPipeline  \n",
    "1. preprocessing:Lowercase, stemming, lemmatization, stopwords  \n",
    "2. N-grams  \n",
    "3. preprocessing: TFiDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Word 2 vec\n",
    "単語をベクトル化させる。単語間のベクトルが特徴を表す。\n",
    "<img src=\"images/wordtovec1.png\" width='400px'>\n",
    "Bag of Wordsとword2vecの比較  \n",
    "<img src=\"images/wordtovec2.png\" width='400px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
