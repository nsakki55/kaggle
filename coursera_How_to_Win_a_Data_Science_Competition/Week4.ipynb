{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost LightGBMのハイパーパラメータ\n",
    "max_depthは7から始めるとよい。数を増やすと過学習の危険がある。  \n",
    "過学習が見られたら、colsample_bytree,feature_fractionの値を下げるとよい  \n",
    "etaが学習係数、num_roundが何本の決定木を作成するかを表す。\n",
    "<img src=\"images/parameter1.png\" width='400px'>\n",
    "## RandamForestのハイパーパラメータ\n",
    "n_estimatorsは多ければよくなる。  \n",
    "criterionではたいていはginiが良い結果になる場合が多いが,entropyが勝つ場合もある。\n",
    "<img src=\"images/parameter1.png\" width='400px'>\n",
    "\n",
    "## Neural Networkのハイパーパラメータ\n",
    "バッチサイズは32,64あたりから始めるのがよい。バッチサイズの調節で過学習を抑制することができる。  \n",
    "dropoutは最初の層に入れると、多くの情報が失われることになる。  \n",
    "隠れ層のユニット数を増やしすぎると、過学習になる。  \n",
    "<img src=\"images/parameter3.png\" width='400px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自分の中でパイプラインを決める\n",
    "hdf5/npy 形式に変えて、読み込みを早くする。  \n",
    "64-bitのデータ型は意味がないので、32-bitsdに形式を変えて、メモリを抑える。  \n",
    "初めに、簡単なモデルでベースラインを作る。\n",
    "<img src=\"images/parameter4.png\" width='400px'>\n",
    "<img src=\"images/parameter5.png\" width='400px'>\n",
    "\n",
    "<img src=\"images/parameter6.png\" width='400px'>\n",
    "モデルごとに、一つのノートブックを作る癖をつける。後からどのモデルを使ったか分かりやすくなる。\n",
    "<img src=\"images/parameter7.png\" width='400px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaggle masterのpipeline\n",
    "まずは自分でEDA、特徴量エンジニアリングを行い、後からkernelを確認する。アンサンブルは最後に行い、それまでの時間は特徴量エンジニアリングやもでリングに多くの時間を割く。\n",
    "<img src=\"images/pipe1.png\" width='400px'>\n",
    "どういった問題なのか、何を求める問題なのかを確認する。データサイズはどれくらい大きいかを確認。テストデータでの評価指標は何かを確認する。以前に同じようなコンペが開かれていないかを確認し、コードを再利用する\n",
    "<img src=\"images/pipe2.png\" width='400px'>\n",
    "ヒスグラムを確認し、訓練データとテストデータが似ているかを確認する。  \n",
    "ターゲット特徴量、時間に対してプロットをする。  \n",
    "特徴量をnumericalにして、相関行列をもとめる\n",
    "<img src=\"images/pipe3.png\" width='400px'>\n",
    "Validationを決めるのは非常に重要な課程。  \n",
    "時間が重要な時間ベースのvalidationを行う  \n",
    "ラベル数に偏りがある場合は、stratifiedをおこなう。  \n",
    "完全にランダムなデータなら、ランダムvalidaitonを行う。  \n",
    "<img src=\"images/pipe4.png\" width='400px'>\n",
    "\n",
    "特徴量エンジニアリングはデータタイプごと異なる戦略をとる。\n",
    "<img src=\"images/pipe5.png\" width='400px'>\n",
    "\n",
    "問題ごとに特徴量エンジニアリングが変わる。その都度、似たようなコンペで行われている特徴量エンジニアリングを参考にする必要がある。\n",
    "<img src=\"images/pipe6.png\" width='400px'>\n",
    "扱う問題ごとに作成するモデルを変える必要がある。この場合も、似たようなコンペでのモデリングを参考にする必要がある。\n",
    "<img src=\"images/pipe7.png\" width='400px'>\n",
    "Kaggleをゲームとして楽しむことを忘れない。仕事として取り組み始めたら休憩したほうがよい。  \n",
    "役に立つと思ったメソッドは、ノートブックとしてまとめておいて、似たようなコンペがでたら、その都度アップデートする。\n",
    "<img src=\"images/pipe8.png\" width='400px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
