{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost LightGBMのハイパーパラメータ\n",
    "max_depthは7から始めるとよい。数を増やすと過学習の危険がある。  \n",
    "過学習が見られたら、colsample_bytree,feature_fractionの値を下げるとよい  \n",
    "etaが学習係数、num_roundが何本の決定木を作成するかを表す。\n",
    "<img src=\"images/parameter1.png\" width='400px'>\n",
    "## RandamForestのハイパーパラメータ\n",
    "n_estimatorsは多ければよくなる。  \n",
    "criterionではたいていはginiが良い結果になる場合が多いが,entropyが勝つ場合もある。\n",
    "<img src=\"images/parameter1.png\" width='400px'>\n",
    "\n",
    "## Neural Networkのハイパーパラメータ\n",
    "バッチサイズは32,64あたりから始めるのがよい。バッチサイズの調節で過学習を抑制することができる。  \n",
    "dropoutは最初の層に入れると、多くの情報が失われることになる。  \n",
    "隠れ層のユニット数を増やしすぎると、過学習になる。  \n",
    "<img src=\"images/parameter3.png\" width='400px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自分の中でパイプラインを決める\n",
    "hdf5/npy 形式に変えて、読み込みを早くする。  \n",
    "64-bitのデータ型は意味がないので、32-bitsdに形式を変えて、メモリを抑える。  \n",
    "初めに、簡単なモデルでベースラインを作る。\n",
    "<img src=\"images/parameter4.png\" width='400px'>\n",
    "<img src=\"images/parameter5.png\" width='400px'>\n",
    "\n",
    "<img src=\"images/parameter6.png\" width='400px'>\n",
    "モデルごとに、一つのノートブックを作る癖をつける。後からどのモデルを使ったか分かりやすくなる。\n",
    "<img src=\"images/parameter7.png\" width='400px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaggle masterのpipeline\n",
    "まずは自分でEDA、特徴量エンジニアリングを行い、後からkernelを確認する。アンサンブルは最後に行い、それまでの時間は特徴量エンジニアリングやもでリングに多くの時間を割く。\n",
    "<img src=\"images/pipe1.png\" width='400px'>\n",
    "どういった問題なのか、何を求める問題なのかを確認する。データサイズはどれくらい大きいかを確認。テストデータでの評価指標は何かを確認する。以前に同じようなコンペが開かれていないかを確認し、コードを再利用する\n",
    "<img src=\"images/pipe2.png\" width='400px'>\n",
    "ヒスグラムを確認し、訓練データとテストデータが似ているかを確認する。  \n",
    "ターゲット特徴量、時間に対してプロットをする。  \n",
    "特徴量をnumericalにして、相関行列をもとめる\n",
    "<img src=\"images/pipe3.png\" width='400px'>\n",
    "Validationを決めるのは非常に重要な課程。  \n",
    "時間が重要な時間ベースのvalidationを行う  \n",
    "ラベル数に偏りがある場合は、stratifiedをおこなう。  \n",
    "完全にランダムなデータなら、ランダムvalidaitonを行う。  \n",
    "<img src=\"images/pipe4.png\" width='400px'>\n",
    "\n",
    "特徴量エンジニアリングはデータタイプごと異なる戦略をとる。\n",
    "<img src=\"images/pipe5.png\" width='400px'>\n",
    "\n",
    "問題ごとに特徴量エンジニアリングが変わる。その都度、似たようなコンペで行われている特徴量エンジニアリングを参考にする必要がある。\n",
    "<img src=\"images/pipe6.png\" width='400px'>\n",
    "扱う問題ごとに作成するモデルを変える必要がある。この場合も、似たようなコンペでのモデリングを参考にする必要がある。\n",
    "<img src=\"images/pipe7.png\" width='400px'>\n",
    "Kaggleをゲームとして楽しむことを忘れない。仕事として取り組み始めたら休憩したほうがよい。  \n",
    "役に立つと思ったメソッドは、ノートブックとしてまとめておいて、似たようなコンペがでたら、その都度アップデートする。\n",
    "<img src=\"images/pipe8.png\" width='400px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "線形分離不可能なデータをクラスタリングすることで、次元を下げる方法\n",
    "<img src=\"images/sne.png\" width='400px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アンサンブル\n",
    "アンサンブル手法にはいくつかあり、平均をとるのが最も単純\n",
    "<img src=\"images/ensenble.png\" width='400px'>\n",
    "各モデルで、うまく予測できている部分が異なることがあるので、複数のモデルを合わせるとお互いの弱みを補うことができる。\n",
    "<img src=\"images/ensenble2.png\" width='400px'>\n",
    "### Averaging \n",
    "単純に平均を取る方法。モデルは各モデルが性能が良かった部分が鈍化されてしまう。\n",
    "<img src=\"images/ensenble3.png\" width='400px'>\n",
    "### Weighted averaging \n",
    "モデルごとに重みづけを行う方法。分類に偏りが出やすい。\n",
    "<img src=\"images/ensenble4.png\" width='400px'>\n",
    "### Conditional averaging \n",
    "分類する区間でモデルを組み合わせる方法。各モデルが得意な部分を担うので、効果が大きい\n",
    "<img src=\"images/ensenble5.png\" width='400px'>\n",
    "\n",
    "### Bagging\n",
    "バギングを行う理由は  \n",
    "1. バイアスによるエラー（underfitting）\n",
    "2. バリアンスによるエラー(overfitting)  \n",
    "bagsが生成するモデル(決定木)の数に対応。bagsの数だけモデルを作成し、最後に各予測を単純に平均をとる。\n",
    "<img src=\"images/ensenble6.png\" width='500px'>\n",
    "\n",
    "### Boosting\n",
    "Baggingとの大きな違いは、新しいモデルを作る際に、前のモデルよりもよい予測ができるように作れれる点  \n",
    "二つの種類のBoostingがある  \n",
    "1. Weighted Based Boosting  \n",
    "各モデルに対して、前回の予測をもとにして重み付けをおこなう。　　\n",
    "重要なパラメータ  \n",
    "・学習係数  \n",
    "・モデルの数(n_estimators)  \n",
    "・入力のモデルのタイプ　boostingはどのモデルでも使用することができる。基本は決定木  \n",
    "・Sub Booosting Type  \n",
    "    ・Ada Boost  \n",
    "    ・Logit Boost\n",
    "<img src=\"images/ensenble7.png\" width='500px'>\n",
    "2. Residual Based Boosting\n",
    "コンペで最も活躍しているブースティング。  \n",
    "予測値と真値の誤差をとり、得られた誤差を次のモデルで予測をする。  \n",
    "重要なパラメータ  \n",
    "    ・学習係数\n",
    "    ・モデルの数(n_estimators)  \n",
    "    ・行のサブサンプリング  \n",
    "    ・列のサブサンプリング  \n",
    "    ・入力モデル　決定木がよい  \n",
    "    ・Sub Boosting Type  \n",
    "    　・Fully gradient based\n",
    "      ・Dart  \n",
    "人気なResidual Basedのブースティング  \n",
    "・XGBoost  \n",
    "・LightGBM  \n",
    "・H\"O's GBM  \n",
    "・Catboost  \n",
    "・sklearn's GBM\n",
    "<img src=\"images/ensenble8.png\" width='500px'>\n",
    "<img src=\"images/ensenble9.png\" width='500px'>\n",
    "\n",
    "### Stacking\n",
    "A (訓練データ)、B(検証データ)、C(テストデータ)を用意し、訓練データから予測した値を特徴量として、検証でータで予測を行い、テストデータの特徴量に加える。\n",
    "<img src=\"images/ensenble10.png\" width='500px'>\n",
    "メタモデルを複数作り、訓練データと検証データを用意する。  \n",
    "各モデルでの予測を行い、予測結果を新しい特徴量として、検証データの学習を行う。最終的に、検証データで学習したモデルでテストデータの予測を行う。  \n",
    "時系列データの場合は、時間を尊重する。  \n",
    "多様なモデルがあることが、スタッキングでは重要になる。  \n",
    "多様性は・異なるアルゴリズムを使うか、・異なる特徴量を入力すると、得られる。  \n",
    "・Nモデル目で、性能は安定化してしまう。\n",
    "<img src=\"images/ensenble11.png\" width='500px'>\n",
    "\n",
    "### StackNet\n",
    "複数のベースモデルから、複数の教師モデルへと、知識の蒸留がなされる。\n",
    "<img src=\"images/ensenble12.png\" width='500px'>\n",
    "ニューラルネットと同じような構造とみなせる。  \n",
    "ニューラルネットの場合は各ノードは単純な活性化関数だが、StackNetは各ノードに様々なモデルを使うことができる。\n",
    "<img src=\"images/ensenble13.png\" width='500px'>\n",
    "<img src=\"images/ensenble14.png\" width='500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ありがたいお言葉\n",
    "<img src=\"images/ensenble15.png\" width='500px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
