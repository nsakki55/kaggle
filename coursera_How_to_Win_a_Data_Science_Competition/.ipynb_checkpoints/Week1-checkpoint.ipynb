{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap of main ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代表的な分類器の概要\n",
    "## Linear model\n",
    "Logistic Regression  \n",
    "SVM\n",
    "最もシンプルな分類は直線でに２種類のデータを分割すること  \n",
    "分類器それぞれに独自のコスト関数が決められている。  \n",
    "非線形分離ができないという問題点を指摘  \n",
    "## Tree-based\n",
    "### 決定木、ランダムフォレスト、勾配ブースティング\n",
    "テーブルデータを用いるときは最も良い選択といえる場合が多い。データを様々な基準で分割することで、クラスわけを行なう。  \n",
    "そのため、たくさんの決定木を考えることができるため、選択肢が増える。  \n",
    "欠点は、決定木はデータの線形関係を捉えることができないため、線形分類可能なデータに対しての性能が悪くなる場合がある。\n",
    "## k-NN-based\n",
    "### k-nearest neighbor\n",
    "新しいデータ点のラベルを予測する際に、新しいデータに最も近いデータのラベルを採用するという、シンプルなアルゴリズム。  \n",
    "近いオブジェクトが最も近いラベルを持っているだろうという、シンプルな考え方\n",
    "## Neural Networks\n",
    "特殊な分類機として取り上げられている。\n",
    "\n",
    "## No Free Lunch Theorem\n",
    "各アルゴリズムは、それぞれの分類タスクに対しては機能するが、他の分類タスクには機能しないというもの。  \n",
    "各分類機は、データの想定に依存しているため。\n",
    "\n",
    "## 結論\n",
    "・silver bulletなアルゴリズムはない  \n",
    "・線形モデルは、特徴量空間を２つに分割する  \n",
    "・決定木は、特徴量空間をボックスに分割する  \n",
    "・k-NNは最も近い点に強く依存する  \n",
    "・ニューラルネットワークは非線形分類モデル  \n",
    "大抵の場合は勾配ブースティングかニューラルネットワークが機能するが、線形分離可能なモデルの場合には別のモデルも使うべき"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature preprocessing and generation with respect to models\n",
    "# 特徴量の事前処理\n",
    "## overview\n",
    "特徴量には、categorical特徴量、numeric特徴量、文字列特徴量、時系列特徴量があり、それぞれに必要な処理がある。  \n",
    "データ前処理は、新しい特徴量を作成するのに必要となる。  \n",
    "・前処理は必ず必要となる  \n",
    "・特徴量生成は強力な技となる  \n",
    "・前処理と特徴量生成は使うモデルに依存する。\n",
    "\n",
    "## Numeric Features\n",
    "### tree-based model\n",
    "決定木はデータのスケールに影響を受けない\n",
    "### non-tree-based-model\n",
    "k-NNやLinear model, Neural Networkは特徴量のスケールに大きく影響を受ける。特にk-NNではサイズの大きい特徴量が重要な特徴量とされてしまう。  \n",
    "すべてのnumeric特徴量に対して、前処理をしなければならない。\n",
    "### MinMaxScaler\n",
    "最大値と最小値を揃えるスケーリング処理  \n",
    "ヒストグラムの形は処理を加えた前後で変化しない。  \n",
    "### StandardScaler\n",
    "標準化の説明。\n",
    "### 外れ値の処理\n",
    "線形分離では、外れ値に多き日引っ張られて、決定境界が変わってしまう。  \n",
    "ヒストグラムで、データの分布を見た時に外れ値があるか確認できる。\n",
    "### Rank Scaler\n",
    "外れ値への対応として有効な手段。特徴量の大きさをランキングした際のindicesを特徴呂とすることで、もとのスケールの違いを軽減させることができる。kNN,Linear model,Nreural Networkに有効な手段。  \n",
    "手作業で外れ値の処理が難しい際に有効な処理となる。  \n",
    "scipy.stats.rankdataで実装可能\n",
    "### log transform\n",
    "np.log(1+x)\n",
    "### 平方根をとる\n",
    "np.sqrt(x+2/3)\n",
    "大きなスケールの特徴量を平均の値に近づけることができる。0に近い値を分類しやすい大きさに変えることができる。\n",
    "## 特徴量生成\n",
    "prior knowledge,EDAを行なうことで、特徴量間の関係性を明らかにし、新しい特徴量の生成を行なう。ex) 面積、価格→単位面積あたりの価格。  \n",
    "特徴量同士の和、積、商を取ることで新しい特徴量を生成する。\n",
    "\n",
    "## Categorical and Oridinal Features\n",
    "categoical特徴量の中には、意図的にランク付けされているものがある。これらの特徴量をordinal Featuresと呼ぶ。ex) Ticket Class:1,2,3　Driver licence:A,B,C,D Education: kindergarden, school, undergraduate, bachelor, master それぞれには順序には意味がある。  \n",
    "label encodingとfrequency encodingは決定木に対して有効な場合が多い\n",
    "### Label Encoding\n",
    "1. Alphabetical (sorted) アルファベット順に数値をつける。記号の順番に意味がある時に有効  \n",
    "[S,C,Q] -> [2,1,3]    \n",
    "sklearnのsklearn.preprcessing.LabelEncoderで実装\n",
    "\n",
    "2. Order of appearence　特徴量が出現した順に番号をつけていく。各記号に大小関係がない場合は有効  \n",
    "[S,C,Q] -> [1,2,3]  \n",
    "Pandas.factorizeで実装可能 \n",
    "\n",
    "### Frequency encoding \n",
    "各特徴量の出現率でエンコードする方法。割合が当てはめられるので特殊.S,C,S,S,S,Q,S,S,S,C,S  \n",
    "[S,C,Q] -> [0.5,0.2,0.3]   \n",
    "pandasのmapで実装可能,titanic.embarked.map(encoding)\n",
    "\n",
    "### one-hot encoding\n",
    "定番のワンほっとエンコーディング。pandas.get_dummiesm skelarn.preprocessing,OneHot-Encoderで実装可能  \n",
    "決定木に対しては有効な手段とは言えない。ユニークな要素が多いcategorical特徴量ではスパースなデータとなり、メモリに乗り切らない可能性がある。  \n",
    "categorical特徴量や、文字列特徴量に有効な処理といえる。  \n",
    "非決定木分類に対してワンホットエンコーディングは有効な場合が多い\n",
    "\n",
    "## 特徴量生成\n",
    "２つ以上のcategorical,文字列特徴量のラベルを合わせて、分類を行なう。ex) pclass(1,2,3), sex(man,female)を合わせたpclass_sex(1_male,2_male...2_female,3_female)特徴量を作成し、ワンホットエンコーディングを行なう\n",
    "\n",
    "## Datetime and Coorinates 特徴量\n",
    "時系列データの処理の方法を概説  \n",
    "\n",
    "### Date and time\n",
    "1. Preriodicity  \n",
    "時間をweek,month,season,year,second,minute,hourなどの期間で分割をする  \n",
    "<img src=\"images/datetime1.png\" width='500px'>\n",
    "2. Time since  \n",
    "特定のイベントからどれほどの時間が経過したかを考える。  \n",
    "<img src=\"images/datetime2.png\" width='500px'>\n",
    "3. Difference between dates\n",
    "datetime_1 - datetime_2 のように２つの時系列データの差を取る。  \n",
    "\n",
    "###  coordinate(座標)\n",
    "1. interestion place from train/test data or addtional data\n",
    "座標上から、興味深い位置を特定する\n",
    "2. center of cluster\n",
    "クラスタリングの中心座標を取得し、距離を計算する\n",
    "3. aggregated statics\n",
    "周囲の場所から統計処理を行なう。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
