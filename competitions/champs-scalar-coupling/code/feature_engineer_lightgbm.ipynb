{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/criskiev/distance-is-all-you-need-lb-1-481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../input'\n",
    "SUBMISSIONS_PATH = './'\n",
    "# use atomic numbers to recode atomic names\n",
    "ATOMIC_NUMBERS = {\n",
    "    'H': 1,\n",
    "    'C': 6,\n",
    "    'N': 7,\n",
    "    'O': 8,\n",
    "    'F': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 120)\n",
    "pd.set_option('display.max_columns', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nagae/.conda/envs/gpu-env/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.809303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.809502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    molecule_index  atom_index_0  atom_index_1  type  scalar_coupling_constant\n",
       "id                                                                            \n",
       "0   1               1             0             1JHC  84.807602               \n",
       "1   1               1             2             2JHH -11.257000               \n",
       "2   1               1             3             2JHH -11.254800               \n",
       "3   1               1             4             2JHH -11.254300               \n",
       "4   1               2             0             1JHC  84.807404               \n",
       "5   1               2             3             2JHH -11.254100               \n",
       "6   1               2             4             2JHH -11.254800               \n",
       "7   1               3             0             1JHC  84.809303               \n",
       "8   1               3             4             2JHH -11.254300               \n",
       "9   1               4             0             1JHC  84.809502               "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dtypes = {\n",
    "    'molecule_name': 'category',\n",
    "    'atom_index_0': 'int8',\n",
    "    'atom_index_1': 'int8',\n",
    "    'type': 'category',\n",
    "    'scalar_coupling_constant': 'float32'\n",
    "}\n",
    "train_csv = pd.read_csv(f'{DATA_PATH}/train.csv', index_col='id', dtype=train_dtypes)\n",
    "train_csv['molecule_index'] = train_csv.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "train_csv = train_csv[['molecule_index', 'atom_index_0', 'atom_index_1', 'type', 'scalar_coupling_constant']]\n",
    "train_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (4658147, 5)\n",
      "Total:  88505177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index                       37265176\n",
       "molecule_index              18632588\n",
       "atom_index_0                4658147 \n",
       "atom_index_1                4658147 \n",
       "type                        4658531 \n",
       "scalar_coupling_constant    18632588\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Shape: ', train_csv.shape)\n",
    "print('Total: ', train_csv.memory_usage().sum())\n",
    "train_csv.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv = pd.read_csv(f'{DATA_PATH}/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4658147</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658148</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658149</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658150</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658151</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658152</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658153</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658154</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658155</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658156</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         molecule_index  atom_index_0  atom_index_1  type\n",
       "id                                                       \n",
       "4658147  4               2             0             2JHC\n",
       "4658148  4               2             1             1JHC\n",
       "4658149  4               2             3             3JHH\n",
       "4658150  4               3             0             1JHC\n",
       "4658151  4               3             1             2JHC\n",
       "4658152  15              3             0             1JHC\n",
       "4658153  15              3             2             3JHC\n",
       "4658154  15              3             4             2JHH\n",
       "4658155  15              3             5             2JHH\n",
       "4658156  15              4             0             1JHC"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = pd.read_csv(f'{DATA_PATH}/test.csv', index_col='id', dtype=train_dtypes)\n",
    "test_csv['molecule_index'] = test_csv['molecule_name'].str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "test_csv = test_csv[['molecule_index', 'atom_index_0', 'atom_index_1', 'type']]\n",
    "test_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>atom</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.011731</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.540815</td>\n",
       "      <td>1.447527</td>\n",
       "      <td>-0.876644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.523814</td>\n",
       "      <td>1.437933</td>\n",
       "      <td>0.906397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.040426</td>\n",
       "      <td>1.024108</td>\n",
       "      <td>0.062564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017257</td>\n",
       "      <td>0.012545</td>\n",
       "      <td>-0.027377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>1.358745</td>\n",
       "      <td>-0.028758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.520278</td>\n",
       "      <td>1.343532</td>\n",
       "      <td>-0.775543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.034360</td>\n",
       "      <td>0.977540</td>\n",
       "      <td>0.007602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   molecule_index  atom_index  atom         x         y         z\n",
       "0  1               0           6    -0.012698  1.085804  0.008001\n",
       "1  1               1           1     0.002150 -0.006031  0.001976\n",
       "2  1               2           1     1.011731  1.463751  0.000277\n",
       "3  1               3           1    -0.540815  1.447527 -0.876644\n",
       "4  1               4           1    -0.523814  1.437933  0.906397\n",
       "5  2               0           7    -0.040426  1.024108  0.062564\n",
       "6  2               1           1     0.017257  0.012545 -0.027377\n",
       "7  2               2           1     0.915789  1.358745 -0.028758\n",
       "8  2               3           1    -0.520278  1.343532 -0.775543\n",
       "9  3               0           8    -0.034360  0.977540  0.007602"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_dtypes = {\n",
    "    'molecule_name': 'category',\n",
    "    'atom_index': 'int8',\n",
    "    'atom': 'category',\n",
    "    'x': 'float32',\n",
    "    'y': 'float32',\n",
    "    'z': 'float32'\n",
    "}\n",
    "structures_csv = pd.read_csv(f'{DATA_PATH}/structures.csv', dtype=structures_dtypes)\n",
    "structures_csv['molecule_index'] = structures_csv.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "structures_csv = structures_csv[['molecule_index', 'atom_index', 'atom', 'x', 'y', 'z']]\n",
    "structures_csv['atom'] = structures_csv['atom'].replace(ATOMIC_NUMBERS).astype('int8')\n",
    "structures_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_type_dataframes(base, structures, coupling_type):\n",
    "    base = base[base['type'] == coupling_type].drop('type', axis=1).copy()\n",
    "    base = base.reset_index()\n",
    "    base['id'] = base['id'].astype('int32')\n",
    "    structures = structures[structures['molecule_index'].isin(base['molecule_index'])]\n",
    "    return base, structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coordinates(base, structures, index):\n",
    "    df = pd.merge(base, structures, how='inner',\n",
    "                  left_on=['molecule_index', f'atom_index_{index}'],\n",
    "                  right_on=['molecule_index', 'atom_index']).drop(['atom_index'], axis=1)\n",
    "    df = df.rename(columns={\n",
    "        'atom': f'atom_{index}',\n",
    "        'x': f'x_{index}',\n",
    "        'y': f'y_{index}',\n",
    "        'z': f'z_{index}'\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_atoms(base, atoms):\n",
    "    df = pd.merge(base, atoms, how='inner',\n",
    "                  on=['molecule_index', 'atom_index_0', 'atom_index_1'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_atoms(base, structures):\n",
    "    df = pd.merge(base, structures, how='left',\n",
    "                  left_on=['molecule_index'],\n",
    "                  right_on=['molecule_index'])\n",
    "    df = df[(df.atom_index_0 != df.atom_index) & (df.atom_index_1 != df.atom_index)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_center(df):\n",
    "    df['x_c'] = ((df['x_1'] + df['x_0']) * np.float32(0.5))\n",
    "    df['y_c'] = ((df['y_1'] + df['y_0']) * np.float32(0.5))\n",
    "    df['z_c'] = ((df['z_1'] + df['z_0']) * np.float32(0.5))\n",
    "\n",
    "def add_distance_to_center(df):\n",
    "    df['d_c'] = ((\n",
    "        (df['x_c'] - df['x'])**np.float32(2) +\n",
    "        (df['y_c'] - df['y'])**np.float32(2) + \n",
    "        (df['z_c'] - df['z'])**np.float32(2)\n",
    "    )**np.float32(0.5))\n",
    "\n",
    "def add_distance_between(df, suffix1, suffix2):\n",
    "    df[f'd_{suffix1}_{suffix2}'] = ((\n",
    "        (df[f'x_{suffix1}'] - df[f'x_{suffix2}'])**np.float32(2) +\n",
    "        (df[f'y_{suffix1}'] - df[f'y_{suffix2}'])**np.float32(2) + \n",
    "        (df[f'z_{suffix1}'] - df[f'z_{suffix2}'])**np.float32(2)\n",
    "    )**np.float32(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distances(df):\n",
    "    n_atoms = 1 + max([int(c.split('_')[1]) for c in df.columns if c.startswith('x_')])\n",
    "    \n",
    "    for i in range(1, n_atoms):\n",
    "        for vi in range(min(4, i)):\n",
    "            add_distance_between(df, i, vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_n_atoms(base, structures):\n",
    "    dfs = structures['molecule_index'].value_counts().rename('n_atoms').to_frame()\n",
    "    return pd.merge(base, dfs, left_on='molecule_index', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_couple_dataframe(some_csv, structures_csv, coupling_type, n_atoms=10):\n",
    "    base, structures = build_type_dataframes(some_csv, structures_csv, coupling_type)\n",
    "    base = add_coordinates(base, structures, 0)\n",
    "    base = add_coordinates(base, structures, 1)\n",
    "    \n",
    "    base = base.drop(['atom_0', 'atom_1'], axis=1)\n",
    "    atoms = base.drop('id', axis=1).copy()\n",
    "    if 'scalar_coupling_constant' in some_csv:\n",
    "        atoms = atoms.drop(['scalar_coupling_constant'], axis=1)\n",
    "        \n",
    "    add_center(atoms)\n",
    "    atoms = atoms.drop(['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1'], axis=1)\n",
    "\n",
    "    atoms = merge_all_atoms(atoms, structures)\n",
    "    \n",
    "    add_distance_to_center(atoms)\n",
    "    \n",
    "    atoms = atoms.drop(['x_c', 'y_c', 'z_c', 'atom_index'], axis=1)\n",
    "    atoms.sort_values(['molecule_index', 'atom_index_0', 'atom_index_1', 'd_c'], inplace=True)\n",
    "    atom_groups = atoms.groupby(['molecule_index', 'atom_index_0', 'atom_index_1'])\n",
    "    atoms['num'] = atom_groups.cumcount() + 2\n",
    "    atoms = atoms.drop(['d_c'], axis=1)\n",
    "    atoms = atoms[atoms['num'] < n_atoms]\n",
    "\n",
    "    atoms = atoms.set_index(['molecule_index', 'atom_index_0', 'atom_index_1', 'num']).unstack()\n",
    "    atoms.columns = [f'{col[0]}_{col[1]}' for col in atoms.columns]\n",
    "    atoms = atoms.reset_index()\n",
    "    \n",
    "    # downcast back to int8\n",
    "    for col in atoms.columns:\n",
    "        if col.startswith('atom_'):\n",
    "            atoms[col] = atoms[col].fillna(0).astype('int8')\n",
    "            \n",
    "    atoms['molecule_index'] = atoms['molecule_index'].astype('int32')\n",
    "    \n",
    "    full = add_atoms(base, atoms)\n",
    "    add_distances(full)\n",
    "    \n",
    "    full.sort_values('id', inplace=True)\n",
    "    \n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_n_atoms(df, n_atoms, four_start=4):\n",
    "    labels = []\n",
    "    for i in range(2, n_atoms):\n",
    "        label = f'atom_{i}'\n",
    "        labels.append(label)\n",
    "\n",
    "    for i in range(n_atoms):\n",
    "        num = min(i, 4) if i < four_start else 4\n",
    "        for j in range(num):\n",
    "            labels.append(f'd_{i}_{j}')\n",
    "    if 'scalar_coupling_constant' in df:\n",
    "        labels.append('scalar_coupling_constant')\n",
    "    return df[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43363, 73)\n",
      "CPU times: user 3.13 s, sys: 79.9 ms, total: 3.21 s\n",
      "Wall time: 854 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full = build_couple_dataframe(train_csv, structures_csv, '1JHN', n_atoms=10)\n",
    "print(full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "      <th>x_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>z_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>z_1</th>\n",
       "      <th>atom_2</th>\n",
       "      <th>atom_3</th>\n",
       "      <th>atom_4</th>\n",
       "      <th>atom_5</th>\n",
       "      <th>atom_6</th>\n",
       "      <th>atom_7</th>\n",
       "      <th>atom_8</th>\n",
       "      <th>atom_9</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "      <th>y_4</th>\n",
       "      <th>y_5</th>\n",
       "      <th>y_6</th>\n",
       "      <th>y_7</th>\n",
       "      <th>y_8</th>\n",
       "      <th>y_9</th>\n",
       "      <th>z_2</th>\n",
       "      <th>z_3</th>\n",
       "      <th>z_4</th>\n",
       "      <th>z_5</th>\n",
       "      <th>z_6</th>\n",
       "      <th>z_7</th>\n",
       "      <th>z_8</th>\n",
       "      <th>z_9</th>\n",
       "      <th>d_1_0</th>\n",
       "      <th>d_2_0</th>\n",
       "      <th>d_2_1</th>\n",
       "      <th>d_3_0</th>\n",
       "      <th>d_3_1</th>\n",
       "      <th>d_3_2</th>\n",
       "      <th>d_4_0</th>\n",
       "      <th>d_4_1</th>\n",
       "      <th>d_4_2</th>\n",
       "      <th>d_4_3</th>\n",
       "      <th>d_5_0</th>\n",
       "      <th>d_5_1</th>\n",
       "      <th>d_5_2</th>\n",
       "      <th>d_5_3</th>\n",
       "      <th>d_6_0</th>\n",
       "      <th>d_6_1</th>\n",
       "      <th>d_6_2</th>\n",
       "      <th>d_6_3</th>\n",
       "      <th>d_7_0</th>\n",
       "      <th>d_7_1</th>\n",
       "      <th>d_7_2</th>\n",
       "      <th>d_7_3</th>\n",
       "      <th>d_8_0</th>\n",
       "      <th>d_8_1</th>\n",
       "      <th>d_8_2</th>\n",
       "      <th>d_8_3</th>\n",
       "      <th>d_9_0</th>\n",
       "      <th>d_9_1</th>\n",
       "      <th>d_9_2</th>\n",
       "      <th>d_9_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32.688900</td>\n",
       "      <td>0.017257</td>\n",
       "      <td>0.012545</td>\n",
       "      <td>-0.027377</td>\n",
       "      <td>-0.040426</td>\n",
       "      <td>1.024108</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>-0.520278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.358745</td>\n",
       "      <td>1.343532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.028758</td>\n",
       "      <td>-0.775543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.017190</td>\n",
       "      <td>1.618523</td>\n",
       "      <td>1.017187</td>\n",
       "      <td>1.618710</td>\n",
       "      <td>1.017208</td>\n",
       "      <td>1.618706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>32.689098</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>1.358745</td>\n",
       "      <td>-0.028758</td>\n",
       "      <td>-0.040426</td>\n",
       "      <td>1.024108</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017257</td>\n",
       "      <td>-0.520278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012545</td>\n",
       "      <td>1.343532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.027377</td>\n",
       "      <td>-0.775543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.017187</td>\n",
       "      <td>1.618523</td>\n",
       "      <td>1.017190</td>\n",
       "      <td>1.618706</td>\n",
       "      <td>1.017208</td>\n",
       "      <td>1.618710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>32.690498</td>\n",
       "      <td>-0.520278</td>\n",
       "      <td>1.343532</td>\n",
       "      <td>-0.775543</td>\n",
       "      <td>-0.040426</td>\n",
       "      <td>1.024108</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.017257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.358745</td>\n",
       "      <td>0.012545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.028758</td>\n",
       "      <td>-0.027377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.017208</td>\n",
       "      <td>1.618706</td>\n",
       "      <td>1.017187</td>\n",
       "      <td>1.618710</td>\n",
       "      <td>1.017190</td>\n",
       "      <td>1.618523</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>55.525200</td>\n",
       "      <td>0.825355</td>\n",
       "      <td>1.885049</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>-0.025900</td>\n",
       "      <td>1.346146</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.908377</td>\n",
       "      <td>0.046467</td>\n",
       "      <td>1.071835</td>\n",
       "      <td>-0.961441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.826796</td>\n",
       "      <td>-0.011743</td>\n",
       "      <td>-0.652588</td>\n",
       "      <td>-0.475004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018920</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>-0.011133</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.007511</td>\n",
       "      <td>1.734777</td>\n",
       "      <td>1.004933</td>\n",
       "      <td>2.050487</td>\n",
       "      <td>1.359838</td>\n",
       "      <td>2.071779</td>\n",
       "      <td>2.549623</td>\n",
       "      <td>2.280430</td>\n",
       "      <td>3.173246</td>\n",
       "      <td>1.209220</td>\n",
       "      <td>2.960154</td>\n",
       "      <td>2.047394</td>\n",
       "      <td>2.302437</td>\n",
       "      <td>1.109295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>54.735901</td>\n",
       "      <td>-0.908377</td>\n",
       "      <td>1.826796</td>\n",
       "      <td>0.018920</td>\n",
       "      <td>-0.025900</td>\n",
       "      <td>1.346146</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.825355</td>\n",
       "      <td>0.046467</td>\n",
       "      <td>-0.961441</td>\n",
       "      <td>1.071835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.885049</td>\n",
       "      <td>-0.011743</td>\n",
       "      <td>-0.475004</td>\n",
       "      <td>-0.652588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>-0.011133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.004933</td>\n",
       "      <td>1.734777</td>\n",
       "      <td>1.007511</td>\n",
       "      <td>2.071779</td>\n",
       "      <td>1.359838</td>\n",
       "      <td>2.050487</td>\n",
       "      <td>2.302437</td>\n",
       "      <td>2.047394</td>\n",
       "      <td>2.960154</td>\n",
       "      <td>1.109295</td>\n",
       "      <td>3.173246</td>\n",
       "      <td>2.280430</td>\n",
       "      <td>2.549623</td>\n",
       "      <td>1.209220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  molecule_index  atom_index_0  atom_index_1  scalar_coupling_constant  \\\n",
       "0  10   2               1             0             32.688900                  \n",
       "1  13   2               2             0             32.689098                  \n",
       "2  15   2               3             0             32.690498                  \n",
       "3  97   12              3             0             55.525200                  \n",
       "4  101  12              4             0             54.735901                  \n",
       "\n",
       "        x_0       y_0       z_0       x_1       y_1       z_1  atom_2  atom_3  \\\n",
       "0  0.017257  0.012545 -0.027377 -0.040426  1.024108  0.062564  1       1        \n",
       "1  0.915789  1.358745 -0.028758 -0.040426  1.024108  0.062564  1       1        \n",
       "2 -0.520278  1.343532 -0.775543 -0.040426  1.024108  0.062564  1       1        \n",
       "3  0.825355  1.885049  0.003738 -0.025900  1.346146  0.008894  1       6        \n",
       "4 -0.908377  1.826796  0.018920 -0.025900  1.346146  0.008894  1       6        \n",
       "\n",
       "   atom_4  atom_5  atom_6  atom_7  atom_8  atom_9       x_2       x_3  \\\n",
       "0  0       0       0       0       0       0       0.915789 -0.520278   \n",
       "1  0       0       0       0       0       0       0.017257 -0.520278   \n",
       "2  0       0       0       0       0       0       0.915789  0.017257   \n",
       "3  8       1       0       0       0       0      -0.908377  0.046467   \n",
       "4  1       8       0       0       0       0       0.825355  0.046467   \n",
       "\n",
       "        x_4       x_5  x_6  x_7  x_8  x_9       y_2       y_3       y_4  \\\n",
       "0 NaN       NaN       NaN  NaN  NaN  NaN   1.358745  1.343532 NaN         \n",
       "1 NaN       NaN       NaN  NaN  NaN  NaN   0.012545  1.343532 NaN         \n",
       "2 NaN       NaN       NaN  NaN  NaN  NaN   1.358745  0.012545 NaN         \n",
       "3  1.071835 -0.961441 NaN  NaN  NaN  NaN   1.826796 -0.011743 -0.652588   \n",
       "4 -0.961441  1.071835 NaN  NaN  NaN  NaN   1.885049 -0.011743 -0.475004   \n",
       "\n",
       "        y_5  y_6  y_7  y_8  y_9       z_2       z_3       z_4       z_5  z_6  \\\n",
       "0 NaN       NaN  NaN  NaN  NaN  -0.028758 -0.775543 NaN       NaN       NaN    \n",
       "1 NaN       NaN  NaN  NaN  NaN  -0.027377 -0.775543 NaN       NaN       NaN    \n",
       "2 NaN       NaN  NaN  NaN  NaN  -0.028758 -0.027377 NaN       NaN       NaN    \n",
       "3 -0.475004 NaN  NaN  NaN  NaN   0.018920  0.001204 -0.011133  0.008074 NaN    \n",
       "4 -0.652588 NaN  NaN  NaN  NaN   0.003738  0.001204  0.008074 -0.011133 NaN    \n",
       "\n",
       "   z_7  z_8  z_9     d_1_0     d_2_0     d_2_1     d_3_0     d_3_1     d_3_2  \\\n",
       "0 NaN  NaN  NaN   1.017190  1.618523  1.017187  1.618710  1.017208  1.618706   \n",
       "1 NaN  NaN  NaN   1.017187  1.618523  1.017190  1.618706  1.017208  1.618710   \n",
       "2 NaN  NaN  NaN   1.017208  1.618706  1.017187  1.618710  1.017190  1.618523   \n",
       "3 NaN  NaN  NaN   1.007511  1.734777  1.004933  2.050487  1.359838  2.071779   \n",
       "4 NaN  NaN  NaN   1.004933  1.734777  1.007511  2.071779  1.359838  2.050487   \n",
       "\n",
       "      d_4_0     d_4_1     d_4_2     d_4_3     d_5_0     d_5_1     d_5_2  \\\n",
       "0 NaN       NaN       NaN       NaN       NaN       NaN       NaN         \n",
       "1 NaN       NaN       NaN       NaN       NaN       NaN       NaN         \n",
       "2 NaN       NaN       NaN       NaN       NaN       NaN       NaN         \n",
       "3  2.549623  2.280430  3.173246  1.209220  2.960154  2.047394  2.302437   \n",
       "4  2.302437  2.047394  2.960154  1.109295  3.173246  2.280430  2.549623   \n",
       "\n",
       "      d_5_3  d_6_0  d_6_1  d_6_2  d_6_3  d_7_0  d_7_1  d_7_2  d_7_3  d_8_0  \\\n",
       "0 NaN       NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN      \n",
       "1 NaN       NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN      \n",
       "2 NaN       NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN      \n",
       "3  1.109295 NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN      \n",
       "4  1.209220 NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN      \n",
       "\n",
       "   d_8_1  d_8_2  d_8_3  d_9_0  d_9_1  d_9_2  d_9_3  \n",
       "0 NaN    NaN    NaN    NaN    NaN    NaN    NaN     \n",
       "1 NaN    NaN    NaN    NaN    NaN    NaN    NaN     \n",
       "2 NaN    NaN    NaN    NaN    NaN    NaN    NaN     \n",
       "3 NaN    NaN    NaN    NaN    NaN    NaN    NaN     \n",
       "4 NaN    NaN    NaN    NaN    NaN    NaN    NaN     "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
       "       'atom_9', 'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0',\n",
       "       'd_4_1', 'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0',\n",
       "       'd_6_1', 'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0',\n",
       "       'd_8_1', 'd_8_2', 'd_8_3', 'd_9_0', 'd_9_1', 'd_9_2', 'd_9_3',\n",
       "       'scalar_coupling_constant'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = take_n_atoms(full, 10)\n",
    "# LightGBM performs better with 0-s then with NaN-s\n",
    "df = df.fillna(0)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34690, 38), (8673, 38), (34690,), (8673,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = df.drop(['scalar_coupling_constant'], axis=1).values.astype('float32')\n",
    "y_data = df['scalar_coupling_constant'].values.astype('float32')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=128)\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration params are copied from @artgor kernel:\n",
    "# https://www.kaggle.com/artgor/brute-force-feature-engineering\n",
    "LGB_PARAMS = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.2,\n",
    "    'num_leaves': 128,\n",
    "    'min_child_samples': 79,\n",
    "    'max_depth': 15,\n",
    "    'subsample_freq': 1,\n",
    "    'subsample': 0.9,\n",
    "    'bagging_seed': 11,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.3,\n",
    "    'colsample_bytree': 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB_PARAMS_1JHN= {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.2,\n",
    "    'num_leaves': 64,\n",
    "    'min_child_samples': 79,\n",
    "    'max_depth': 12,\n",
    "    'subsample_freq': 1,\n",
    "    'subsample': 0.9,\n",
    "    'bagging_seed': 11,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.3,\n",
    "    'colsample_bytree': 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.373664\tvalid_1's l1: 0.48981\n",
      "[200]\ttraining's l1: 0.296557\tvalid_1's l1: 0.446742\n",
      "[300]\ttraining's l1: 0.247452\tvalid_1's l1: 0.424625\n",
      "[400]\ttraining's l1: 0.210984\tvalid_1's l1: 0.410619\n",
      "[500]\ttraining's l1: 0.183504\tvalid_1's l1: 0.400631\n",
      "[600]\ttraining's l1: 0.158901\tvalid_1's l1: 0.393269\n",
      "[700]\ttraining's l1: 0.139522\tvalid_1's l1: 0.387746\n",
      "[800]\ttraining's l1: 0.123854\tvalid_1's l1: 0.383704\n",
      "[900]\ttraining's l1: 0.11068\tvalid_1's l1: 0.380712\n",
      "[1000]\ttraining's l1: 0.099639\tvalid_1's l1: 0.378078\n",
      "[1100]\ttraining's l1: 0.0903025\tvalid_1's l1: 0.376126\n",
      "[1200]\ttraining's l1: 0.0820192\tvalid_1's l1: 0.37463\n",
      "[1300]\ttraining's l1: 0.0746408\tvalid_1's l1: 0.373125\n",
      "[1400]\ttraining's l1: 0.0684193\tvalid_1's l1: 0.371902\n",
      "[1500]\ttraining's l1: 0.0625234\tvalid_1's l1: 0.370788\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\ttraining's l1: 0.0625234\tvalid_1's l1: 0.370788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.9921234999423947"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LGBMRegressor(**LGB_PARAMS, n_estimators=1500, n_jobs = -1)\n",
    "model.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='mae',\n",
    "        verbose=100, early_stopping_rounds=200)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "np.log(mean_absolute_error(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = list(df.columns)\n",
    "cols.remove('scalar_coupling_constant')\n",
    "cols\n",
    "df_importance = pd.DataFrame({'feature': cols, 'importance': model.feature_importances_})\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=df_importance.sort_values('importance', ascending=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_x_y_data(some_csv, coupling_type, n_atoms):\n",
    "    full = build_couple_dataframe(some_csv, structures_csv, coupling_type, n_atoms=n_atoms)\n",
    "    \n",
    "    df = take_n_atoms(full, n_atoms)\n",
    "    df = df.fillna(0)\n",
    "    print(df.columns)\n",
    "    \n",
    "    if 'scalar_coupling_constant' in df:\n",
    "        X_data = df.drop(['scalar_coupling_constant'], axis=1).values.astype('float32')\n",
    "        y_data = df['scalar_coupling_constant'].values.astype('float32')\n",
    "    else:\n",
    "        X_data = df.values.astype('float32')\n",
    "        y_data = None\n",
    "        \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def make_nn(input_dim):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(64,input_dim=input_dim,activation='relu'))\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    model.compile(optimizer='rmsprop',loss='mean_absolute_error',metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_for_one_coupling_type(coupling_type, submission, n_atoms, n_folds=5, n_splits=5, random_state=128):\n",
    "    print(f'*** Training Model for {coupling_type} ***')\n",
    "    \n",
    "    X_data, y_data = build_x_y_data(train_csv, coupling_type, n_atoms)\n",
    "    X_test, _ = build_x_y_data(test_csv, coupling_type, n_atoms)\n",
    "    y_pred = np.zeros(X_test.shape[0], dtype='float32')\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc=StandardScaler()\n",
    "    X_data=sc.fit_transform(X_data)\n",
    "    X_test=sc.transform(X_test)\n",
    "    \n",
    "    cv_score = 0\n",
    "    \n",
    "    if n_folds > n_splits:\n",
    "        n_splits = n_folds\n",
    "    \n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(X_data, y_data)):\n",
    "        if fold >= n_folds:\n",
    "            break\n",
    "\n",
    "        X_train, X_val = X_data[train_index], X_data[val_index]\n",
    "        y_train, y_val = y_data[train_index], y_data[val_index]\n",
    "        \n",
    "        #if coupling_type=='1JHN':\n",
    "        #    model = LGBMRegressor(**LGB_PARAMS_1JHN, n_estimators=1500, n_jobs = -1)\n",
    "        #else:\n",
    "        #    model = LGBMRegressor(**LGB_PARAMS, n_estimators=1500, n_jobs = -1)\n",
    "        #model.fit(X_train, y_train, \n",
    "        #    eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='mae',\n",
    "        #    verbose=100, early_stopping_rounds=200)\n",
    "        \n",
    "        model=make_nn(X_train.shape[1])\n",
    "        model.fit(X_train,y_train,epochs=50,batch_size=128,validation_data=[X_val,y_val])\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_score = np.log(mean_absolute_error(y_val, y_val_pred))\n",
    "        print(f'{coupling_type} Fold {fold}, logMAE: {val_score}')\n",
    "        \n",
    "        cv_score += val_score / n_folds\n",
    "        pred=model.predict(X_test)\n",
    "        pred=pred.reshape(X_test.shape[0],)\n",
    "        print(y_pred.shape)\n",
    "        y_pred += pred / n_folds\n",
    "        print(cv_score)\n",
    "        \n",
    "        \n",
    "    submission.loc[test_csv['type'] == coupling_type, 'scalar_coupling_constant'] = y_pred\n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training Model for 1JHN ***\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'd_1_0', 'd_2_0',\n",
      "       'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1', 'd_4_2', 'd_4_3',\n",
      "       'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1', 'd_6_2', 'd_6_3',\n",
      "       'scalar_coupling_constant'],\n",
      "      dtype='object')\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'd_1_0', 'd_2_0',\n",
      "       'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1', 'd_4_2', 'd_4_3',\n",
      "       'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1', 'd_6_2', 'd_6_3'],\n",
      "      dtype='object')\n",
      "Train on 34690 samples, validate on 8673 samples\n",
      "Epoch 1/50\n",
      "34690/34690 [==============================] - 1s 18us/step - loss: 23.7609 - mean_absolute_error: 23.7609 - val_loss: 4.4902 - val_mean_absolute_error: 4.4902\n",
      "Epoch 2/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 2.5375 - mean_absolute_error: 2.5375 - val_loss: 1.9637 - val_mean_absolute_error: 1.9637\n",
      "Epoch 3/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.6181 - mean_absolute_error: 1.6181 - val_loss: 1.5631 - val_mean_absolute_error: 1.5631\n",
      "Epoch 4/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.3969 - mean_absolute_error: 1.3969 - val_loss: 1.6993 - val_mean_absolute_error: 1.6993\n",
      "Epoch 5/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.2899 - mean_absolute_error: 1.2899 - val_loss: 1.5234 - val_mean_absolute_error: 1.5234\n",
      "Epoch 6/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.2252 - mean_absolute_error: 1.2252 - val_loss: 1.4686 - val_mean_absolute_error: 1.4686\n",
      "Epoch 7/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.1751 - mean_absolute_error: 1.1751 - val_loss: 1.4231 - val_mean_absolute_error: 1.4231\n",
      "Epoch 8/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.1341 - mean_absolute_error: 1.1341 - val_loss: 1.2857 - val_mean_absolute_error: 1.2857\n",
      "Epoch 9/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.0961 - mean_absolute_error: 1.0961 - val_loss: 1.6725 - val_mean_absolute_error: 1.6725\n",
      "Epoch 10/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.0679 - mean_absolute_error: 1.0679 - val_loss: 1.3088 - val_mean_absolute_error: 1.3088\n",
      "Epoch 11/50\n",
      "34690/34690 [==============================] - 0s 13us/step - loss: 1.0427 - mean_absolute_error: 1.0427 - val_loss: 1.1522 - val_mean_absolute_error: 1.1522\n",
      "Epoch 12/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.0221 - mean_absolute_error: 1.0221 - val_loss: 1.3397 - val_mean_absolute_error: 1.3397\n",
      "Epoch 13/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.0018 - mean_absolute_error: 1.0018 - val_loss: 1.3013 - val_mean_absolute_error: 1.3013\n",
      "Epoch 14/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9872 - mean_absolute_error: 0.9872 - val_loss: 1.1040 - val_mean_absolute_error: 1.1040\n",
      "Epoch 15/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9699 - mean_absolute_error: 0.9699 - val_loss: 1.4881 - val_mean_absolute_error: 1.4881\n",
      "Epoch 16/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9611 - mean_absolute_error: 0.9611 - val_loss: 1.4439 - val_mean_absolute_error: 1.4439\n",
      "Epoch 17/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.9458 - mean_absolute_error: 0.9458 - val_loss: 1.0842 - val_mean_absolute_error: 1.0842\n",
      "Epoch 18/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9357 - mean_absolute_error: 0.9357 - val_loss: 1.2713 - val_mean_absolute_error: 1.2713\n",
      "Epoch 19/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9243 - mean_absolute_error: 0.9243 - val_loss: 1.2753 - val_mean_absolute_error: 1.2753\n",
      "Epoch 20/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9176 - mean_absolute_error: 0.9176 - val_loss: 1.2231 - val_mean_absolute_error: 1.2231\n",
      "Epoch 21/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9109 - mean_absolute_error: 0.9109 - val_loss: 1.2500 - val_mean_absolute_error: 1.2500\n",
      "Epoch 22/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9032 - mean_absolute_error: 0.9032 - val_loss: 1.3534 - val_mean_absolute_error: 1.3534\n",
      "Epoch 23/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8992 - mean_absolute_error: 0.8992 - val_loss: 1.1324 - val_mean_absolute_error: 1.1324\n",
      "Epoch 24/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8941 - mean_absolute_error: 0.8941 - val_loss: 1.1429 - val_mean_absolute_error: 1.1429\n",
      "Epoch 25/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8876 - mean_absolute_error: 0.8876 - val_loss: 1.2436 - val_mean_absolute_error: 1.2436\n",
      "Epoch 26/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8786 - mean_absolute_error: 0.8786 - val_loss: 1.3461 - val_mean_absolute_error: 1.3461\n",
      "Epoch 27/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8734 - mean_absolute_error: 0.8734 - val_loss: 1.2737 - val_mean_absolute_error: 1.2737\n",
      "Epoch 28/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8693 - mean_absolute_error: 0.8693 - val_loss: 1.4898 - val_mean_absolute_error: 1.4898\n",
      "Epoch 29/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8644 - mean_absolute_error: 0.8644 - val_loss: 1.1952 - val_mean_absolute_error: 1.1952\n",
      "Epoch 30/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8599 - mean_absolute_error: 0.8599 - val_loss: 0.8829 - val_mean_absolute_error: 0.8829\n",
      "Epoch 31/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8567 - mean_absolute_error: 0.8567 - val_loss: 0.9712 - val_mean_absolute_error: 0.9712\n",
      "Epoch 32/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8513 - mean_absolute_error: 0.8513 - val_loss: 0.9918 - val_mean_absolute_error: 0.9918\n",
      "Epoch 33/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8463 - mean_absolute_error: 0.8463 - val_loss: 1.3097 - val_mean_absolute_error: 1.3097\n",
      "Epoch 34/50\n",
      "34690/34690 [==============================] - 0s 13us/step - loss: 0.8444 - mean_absolute_error: 0.8444 - val_loss: 1.1421 - val_mean_absolute_error: 1.1421\n",
      "Epoch 35/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8378 - mean_absolute_error: 0.8378 - val_loss: 1.3075 - val_mean_absolute_error: 1.3075\n",
      "Epoch 36/50\n",
      "34690/34690 [==============================] - 0s 13us/step - loss: 0.8352 - mean_absolute_error: 0.8352 - val_loss: 1.1354 - val_mean_absolute_error: 1.1354\n",
      "Epoch 37/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8312 - mean_absolute_error: 0.8312 - val_loss: 1.1828 - val_mean_absolute_error: 1.1828\n",
      "Epoch 38/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8295 - mean_absolute_error: 0.8295 - val_loss: 1.7330 - val_mean_absolute_error: 1.7330\n",
      "Epoch 39/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8243 - mean_absolute_error: 0.8243 - val_loss: 1.4858 - val_mean_absolute_error: 1.4858\n",
      "Epoch 40/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8262 - mean_absolute_error: 0.8262 - val_loss: 1.4001 - val_mean_absolute_error: 1.4001\n",
      "Epoch 41/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8212 - mean_absolute_error: 0.8212 - val_loss: 0.9035 - val_mean_absolute_error: 0.9035\n",
      "Epoch 42/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8176 - mean_absolute_error: 0.8176 - val_loss: 1.0942 - val_mean_absolute_error: 1.0942\n",
      "Epoch 43/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8155 - mean_absolute_error: 0.8155 - val_loss: 1.1817 - val_mean_absolute_error: 1.1817\n",
      "Epoch 44/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8114 - mean_absolute_error: 0.8114 - val_loss: 1.0344 - val_mean_absolute_error: 1.0344\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8078 - mean_absolute_error: 0.8078 - val_loss: 1.2088 - val_mean_absolute_error: 1.2088\n",
      "Epoch 46/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8042 - mean_absolute_error: 0.8042 - val_loss: 1.3857 - val_mean_absolute_error: 1.3857\n",
      "Epoch 47/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8080 - mean_absolute_error: 0.8080 - val_loss: 1.0109 - val_mean_absolute_error: 1.0109\n",
      "Epoch 48/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8017 - mean_absolute_error: 0.8017 - val_loss: 1.3016 - val_mean_absolute_error: 1.3016\n",
      "Epoch 49/50\n",
      "34690/34690 [==============================] - 0s 10us/step - loss: 0.7999 - mean_absolute_error: 0.7999 - val_loss: 1.5651 - val_mean_absolute_error: 1.5651\n",
      "Epoch 50/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.7986 - mean_absolute_error: 0.7986 - val_loss: 1.1547 - val_mean_absolute_error: 1.1547\n",
      "1JHN Fold 0, logMAE: 0.14386363327503204\n",
      "(24195,)\n",
      "0.04795454442501068\n",
      "Train on 34690 samples, validate on 8673 samples\n",
      "Epoch 1/50\n",
      "34690/34690 [==============================] - 1s 19us/step - loss: 17.9125 - mean_absolute_error: 17.9125 - val_loss: 2.9808 - val_mean_absolute_error: 2.9808\n",
      "Epoch 2/50\n",
      "34690/34690 [==============================] - 0s 13us/step - loss: 2.1427 - mean_absolute_error: 2.1427 - val_loss: 2.0264 - val_mean_absolute_error: 2.0264\n",
      "Epoch 3/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 1.5700 - mean_absolute_error: 1.5700 - val_loss: 1.5098 - val_mean_absolute_error: 1.5098\n",
      "Epoch 4/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 1.3764 - mean_absolute_error: 1.3764 - val_loss: 1.5807 - val_mean_absolute_error: 1.5807\n",
      "Epoch 5/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 1.2835 - mean_absolute_error: 1.2835 - val_loss: 1.5640 - val_mean_absolute_error: 1.5640\n",
      "Epoch 6/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.2105 - mean_absolute_error: 1.2105 - val_loss: 1.2623 - val_mean_absolute_error: 1.2623\n",
      "Epoch 7/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 1.1547 - mean_absolute_error: 1.1547 - val_loss: 1.2636 - val_mean_absolute_error: 1.2636\n",
      "Epoch 8/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.1096 - mean_absolute_error: 1.1096 - val_loss: 1.2944 - val_mean_absolute_error: 1.2944\n",
      "Epoch 9/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 1.0702 - mean_absolute_error: 1.0702 - val_loss: 1.1732 - val_mean_absolute_error: 1.1732\n",
      "Epoch 10/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 1.0432 - mean_absolute_error: 1.0432 - val_loss: 1.4327 - val_mean_absolute_error: 1.4327\n",
      "Epoch 11/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.0202 - mean_absolute_error: 1.0202 - val_loss: 1.1207 - val_mean_absolute_error: 1.1207\n",
      "Epoch 12/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9976 - mean_absolute_error: 0.9976 - val_loss: 1.3283 - val_mean_absolute_error: 1.3283\n",
      "Epoch 13/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9840 - mean_absolute_error: 0.9840 - val_loss: 1.3325 - val_mean_absolute_error: 1.3325\n",
      "Epoch 14/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9602 - mean_absolute_error: 0.9602 - val_loss: 1.0447 - val_mean_absolute_error: 1.0447\n",
      "Epoch 15/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9480 - mean_absolute_error: 0.9480 - val_loss: 1.1773 - val_mean_absolute_error: 1.1773\n",
      "Epoch 16/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9362 - mean_absolute_error: 0.9362 - val_loss: 1.5041 - val_mean_absolute_error: 1.5041\n",
      "Epoch 17/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9233 - mean_absolute_error: 0.9233 - val_loss: 1.5677 - val_mean_absolute_error: 1.5677\n",
      "Epoch 18/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9158 - mean_absolute_error: 0.9158 - val_loss: 1.1974 - val_mean_absolute_error: 1.1974\n",
      "Epoch 19/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9027 - mean_absolute_error: 0.9027 - val_loss: 1.1836 - val_mean_absolute_error: 1.1836\n",
      "Epoch 20/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8967 - mean_absolute_error: 0.8967 - val_loss: 0.9503 - val_mean_absolute_error: 0.9503\n",
      "Epoch 21/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8869 - mean_absolute_error: 0.8869 - val_loss: 1.1622 - val_mean_absolute_error: 1.1622\n",
      "Epoch 22/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8814 - mean_absolute_error: 0.8814 - val_loss: 0.9498 - val_mean_absolute_error: 0.9498\n",
      "Epoch 23/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8707 - mean_absolute_error: 0.8707 - val_loss: 1.7013 - val_mean_absolute_error: 1.7013\n",
      "Epoch 24/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8663 - mean_absolute_error: 0.8663 - val_loss: 1.2870 - val_mean_absolute_error: 1.2870\n",
      "Epoch 25/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8634 - mean_absolute_error: 0.8634 - val_loss: 1.0532 - val_mean_absolute_error: 1.0532\n",
      "Epoch 26/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8552 - mean_absolute_error: 0.8552 - val_loss: 1.5722 - val_mean_absolute_error: 1.5722\n",
      "Epoch 27/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8514 - mean_absolute_error: 0.8514 - val_loss: 1.0202 - val_mean_absolute_error: 1.0202\n",
      "Epoch 28/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8449 - mean_absolute_error: 0.8449 - val_loss: 1.1376 - val_mean_absolute_error: 1.1376\n",
      "Epoch 29/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8423 - mean_absolute_error: 0.8423 - val_loss: 1.2958 - val_mean_absolute_error: 1.2958\n",
      "Epoch 30/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8339 - mean_absolute_error: 0.8339 - val_loss: 1.1805 - val_mean_absolute_error: 1.1805\n",
      "Epoch 31/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8352 - mean_absolute_error: 0.8352 - val_loss: 1.2629 - val_mean_absolute_error: 1.2629\n",
      "Epoch 32/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8271 - mean_absolute_error: 0.8271 - val_loss: 0.8441 - val_mean_absolute_error: 0.8441\n",
      "Epoch 33/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8248 - mean_absolute_error: 0.8248 - val_loss: 1.1802 - val_mean_absolute_error: 1.1802\n",
      "Epoch 34/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8184 - mean_absolute_error: 0.8184 - val_loss: 1.2785 - val_mean_absolute_error: 1.2785\n",
      "Epoch 35/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8184 - mean_absolute_error: 0.8184 - val_loss: 1.4167 - val_mean_absolute_error: 1.4167\n",
      "Epoch 36/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8136 - mean_absolute_error: 0.8136 - val_loss: 1.2737 - val_mean_absolute_error: 1.2737\n",
      "Epoch 37/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8123 - mean_absolute_error: 0.8123 - val_loss: 0.8619 - val_mean_absolute_error: 0.8619\n",
      "Epoch 38/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8049 - mean_absolute_error: 0.8049 - val_loss: 0.9106 - val_mean_absolute_error: 0.9106\n",
      "Epoch 39/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8063 - mean_absolute_error: 0.8063 - val_loss: 0.8721 - val_mean_absolute_error: 0.8721\n",
      "Epoch 40/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.7997 - mean_absolute_error: 0.7997 - val_loss: 1.1983 - val_mean_absolute_error: 1.1983\n",
      "Epoch 41/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.7974 - mean_absolute_error: 0.7974 - val_loss: 1.3130 - val_mean_absolute_error: 1.3130\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.7956 - mean_absolute_error: 0.7956 - val_loss: 1.1607 - val_mean_absolute_error: 1.1607\n",
      "Epoch 43/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.7917 - mean_absolute_error: 0.7917 - val_loss: 1.4984 - val_mean_absolute_error: 1.4984\n",
      "Epoch 44/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.7910 - mean_absolute_error: 0.7910 - val_loss: 1.1863 - val_mean_absolute_error: 1.1863\n",
      "Epoch 45/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.7884 - mean_absolute_error: 0.7884 - val_loss: 0.9977 - val_mean_absolute_error: 0.9977\n",
      "Epoch 46/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.7802 - mean_absolute_error: 0.7802 - val_loss: 1.3543 - val_mean_absolute_error: 1.3543\n",
      "Epoch 47/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.7864 - mean_absolute_error: 0.7864 - val_loss: 1.6040 - val_mean_absolute_error: 1.6040\n",
      "Epoch 48/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.7850 - mean_absolute_error: 0.7850 - val_loss: 1.2382 - val_mean_absolute_error: 1.2382\n",
      "Epoch 49/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.7822 - mean_absolute_error: 0.7822 - val_loss: 1.0636 - val_mean_absolute_error: 1.0636\n",
      "Epoch 50/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.7796 - mean_absolute_error: 0.7796 - val_loss: 1.1591 - val_mean_absolute_error: 1.1591\n",
      "1JHN Fold 1, logMAE: 0.1476697027683258\n",
      "(24195,)\n",
      "0.09717777868111929\n",
      "Train on 34690 samples, validate on 8673 samples\n",
      "Epoch 1/50\n",
      "34690/34690 [==============================] - 1s 20us/step - loss: 15.5454 - mean_absolute_error: 15.5454 - val_loss: 2.6056 - val_mean_absolute_error: 2.6056\n",
      "Epoch 2/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.7885 - mean_absolute_error: 1.7885 - val_loss: 1.7801 - val_mean_absolute_error: 1.7801\n",
      "Epoch 3/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.4420 - mean_absolute_error: 1.4420 - val_loss: 1.5778 - val_mean_absolute_error: 1.5778\n",
      "Epoch 4/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 1.3133 - mean_absolute_error: 1.3133 - val_loss: 1.2690 - val_mean_absolute_error: 1.2690\n",
      "Epoch 5/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.2359 - mean_absolute_error: 1.2359 - val_loss: 1.5813 - val_mean_absolute_error: 1.5813\n",
      "Epoch 6/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.1800 - mean_absolute_error: 1.1800 - val_loss: 1.1855 - val_mean_absolute_error: 1.1855\n",
      "Epoch 7/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.1315 - mean_absolute_error: 1.1315 - val_loss: 1.5793 - val_mean_absolute_error: 1.5793\n",
      "Epoch 8/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.0947 - mean_absolute_error: 1.0947 - val_loss: 1.4000 - val_mean_absolute_error: 1.4000\n",
      "Epoch 9/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.0636 - mean_absolute_error: 1.0636 - val_loss: 1.2240 - val_mean_absolute_error: 1.2240\n",
      "Epoch 10/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.0361 - mean_absolute_error: 1.0361 - val_loss: 1.5221 - val_mean_absolute_error: 1.5221\n",
      "Epoch 11/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 1.0127 - mean_absolute_error: 1.0127 - val_loss: 1.4862 - val_mean_absolute_error: 1.4862\n",
      "Epoch 12/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9908 - mean_absolute_error: 0.9908 - val_loss: 1.2152 - val_mean_absolute_error: 1.2152\n",
      "Epoch 13/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9757 - mean_absolute_error: 0.9757 - val_loss: 1.3470 - val_mean_absolute_error: 1.3470\n",
      "Epoch 14/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9608 - mean_absolute_error: 0.9608 - val_loss: 1.7450 - val_mean_absolute_error: 1.7450\n",
      "Epoch 15/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9445 - mean_absolute_error: 0.9445 - val_loss: 0.9567 - val_mean_absolute_error: 0.9567\n",
      "Epoch 16/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9315 - mean_absolute_error: 0.9315 - val_loss: 1.3556 - val_mean_absolute_error: 1.3556\n",
      "Epoch 17/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9201 - mean_absolute_error: 0.9201 - val_loss: 1.0196 - val_mean_absolute_error: 1.0196\n",
      "Epoch 18/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9129 - mean_absolute_error: 0.9129 - val_loss: 0.9277 - val_mean_absolute_error: 0.9277\n",
      "Epoch 19/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.9021 - mean_absolute_error: 0.9021 - val_loss: 1.4747 - val_mean_absolute_error: 1.4747\n",
      "Epoch 20/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8988 - mean_absolute_error: 0.8988 - val_loss: 0.9690 - val_mean_absolute_error: 0.9690\n",
      "Epoch 21/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8849 - mean_absolute_error: 0.8849 - val_loss: 1.2026 - val_mean_absolute_error: 1.2026\n",
      "Epoch 22/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8785 - mean_absolute_error: 0.8785 - val_loss: 1.3471 - val_mean_absolute_error: 1.3471\n",
      "Epoch 23/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8758 - mean_absolute_error: 0.8758 - val_loss: 1.2478 - val_mean_absolute_error: 1.2478\n",
      "Epoch 24/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8665 - mean_absolute_error: 0.8665 - val_loss: 0.9554 - val_mean_absolute_error: 0.9554\n",
      "Epoch 25/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8614 - mean_absolute_error: 0.8614 - val_loss: 1.2688 - val_mean_absolute_error: 1.2688\n",
      "Epoch 26/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8606 - mean_absolute_error: 0.8606 - val_loss: 1.2983 - val_mean_absolute_error: 1.2983\n",
      "Epoch 27/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8514 - mean_absolute_error: 0.8514 - val_loss: 1.5695 - val_mean_absolute_error: 1.5695\n",
      "Epoch 28/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8495 - mean_absolute_error: 0.8495 - val_loss: 0.9457 - val_mean_absolute_error: 0.9457\n",
      "Epoch 29/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8442 - mean_absolute_error: 0.8442 - val_loss: 1.3617 - val_mean_absolute_error: 1.3617\n",
      "Epoch 30/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8401 - mean_absolute_error: 0.8401 - val_loss: 1.1736 - val_mean_absolute_error: 1.1736\n",
      "Epoch 31/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8401 - mean_absolute_error: 0.8401 - val_loss: 1.2101 - val_mean_absolute_error: 1.2101\n",
      "Epoch 32/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8349 - mean_absolute_error: 0.8349 - val_loss: 1.0459 - val_mean_absolute_error: 1.0459\n",
      "Epoch 33/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8321 - mean_absolute_error: 0.8321 - val_loss: 1.4400 - val_mean_absolute_error: 1.4400\n",
      "Epoch 34/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8255 - mean_absolute_error: 0.8255 - val_loss: 1.4460 - val_mean_absolute_error: 1.4460\n",
      "Epoch 35/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8242 - mean_absolute_error: 0.8242 - val_loss: 1.1816 - val_mean_absolute_error: 1.1816\n",
      "Epoch 36/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8202 - mean_absolute_error: 0.8202 - val_loss: 0.9612 - val_mean_absolute_error: 0.9612\n",
      "Epoch 37/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8211 - mean_absolute_error: 0.8211 - val_loss: 0.9947 - val_mean_absolute_error: 0.9947\n",
      "Epoch 38/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8122 - mean_absolute_error: 0.8122 - val_loss: 0.9972 - val_mean_absolute_error: 0.9972\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8112 - mean_absolute_error: 0.8112 - val_loss: 0.9199 - val_mean_absolute_error: 0.9199\n",
      "Epoch 40/50\n",
      "34690/34690 [==============================] - 0s 12us/step - loss: 0.8104 - mean_absolute_error: 0.8104 - val_loss: 1.2279 - val_mean_absolute_error: 1.2279\n",
      "Epoch 41/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8065 - mean_absolute_error: 0.8065 - val_loss: 1.3752 - val_mean_absolute_error: 1.3752\n",
      "Epoch 42/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8049 - mean_absolute_error: 0.8049 - val_loss: 1.1116 - val_mean_absolute_error: 1.1116\n",
      "Epoch 43/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.8060 - mean_absolute_error: 0.8060 - val_loss: 0.8663 - val_mean_absolute_error: 0.8663\n",
      "Epoch 44/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.7987 - mean_absolute_error: 0.7987 - val_loss: 1.2377 - val_mean_absolute_error: 1.2377\n",
      "Epoch 45/50\n",
      "34690/34690 [==============================] - 0s 13us/step - loss: 0.7998 - mean_absolute_error: 0.7998 - val_loss: 0.8988 - val_mean_absolute_error: 0.8988\n",
      "Epoch 46/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.7941 - mean_absolute_error: 0.7941 - val_loss: 1.0646 - val_mean_absolute_error: 1.0646\n",
      "Epoch 47/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.7972 - mean_absolute_error: 0.7972 - val_loss: 0.9962 - val_mean_absolute_error: 0.9962\n",
      "Epoch 48/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.7949 - mean_absolute_error: 0.7949 - val_loss: 0.9977 - val_mean_absolute_error: 0.9977\n",
      "Epoch 49/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.7906 - mean_absolute_error: 0.7906 - val_loss: 1.2418 - val_mean_absolute_error: 1.2418\n",
      "Epoch 50/50\n",
      "34690/34690 [==============================] - 0s 11us/step - loss: 0.7927 - mean_absolute_error: 0.7927 - val_loss: 0.9670 - val_mean_absolute_error: 0.9670\n",
      "1JHN Fold 2, logMAE: -0.033529654145240784\n",
      "(24195,)\n",
      "0.08600122729937236\n",
      "*** Training Model for 1JHC ***\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'atom_9', 'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0',\n",
      "       'd_4_1', 'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0',\n",
      "       'd_6_1', 'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0',\n",
      "       'd_8_1', 'd_8_2', 'd_8_3', 'd_9_0', 'd_9_1', 'd_9_2', 'd_9_3',\n",
      "       'scalar_coupling_constant'],\n",
      "      dtype='object')\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'atom_9', 'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0',\n",
      "       'd_4_1', 'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0',\n",
      "       'd_6_1', 'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0',\n",
      "       'd_8_1', 'd_8_2', 'd_8_3', 'd_9_0', 'd_9_1', 'd_9_2', 'd_9_3'],\n",
      "      dtype='object')\n",
      "Train on 567532 samples, validate on 141884 samples\n",
      "Epoch 1/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 6.1079 - mean_absolute_error: 6.1079 - val_loss: 2.5029 - val_mean_absolute_error: 2.5029\n",
      "Epoch 2/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 2.3214 - mean_absolute_error: 2.3214 - val_loss: 2.2332 - val_mean_absolute_error: 2.2332\n",
      "Epoch 3/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 2.1717 - mean_absolute_error: 2.1717 - val_loss: 2.0850 - val_mean_absolute_error: 2.0850\n",
      "Epoch 4/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 2.0869 - mean_absolute_error: 2.0869 - val_loss: 2.1216 - val_mean_absolute_error: 2.1216\n",
      "Epoch 5/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 2.0301 - mean_absolute_error: 2.0301 - val_loss: 1.9632 - val_mean_absolute_error: 1.9632\n",
      "Epoch 6/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.9863 - mean_absolute_error: 1.9863 - val_loss: 1.9689 - val_mean_absolute_error: 1.9689\n",
      "Epoch 7/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.9490 - mean_absolute_error: 1.9490 - val_loss: 1.9405 - val_mean_absolute_error: 1.9405\n",
      "Epoch 8/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.9171 - mean_absolute_error: 1.9171 - val_loss: 1.8621 - val_mean_absolute_error: 1.8621\n",
      "Epoch 9/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.8912 - mean_absolute_error: 1.8912 - val_loss: 1.8648 - val_mean_absolute_error: 1.8648\n",
      "Epoch 10/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.8714 - mean_absolute_error: 1.8714 - val_loss: 2.0635 - val_mean_absolute_error: 2.0635\n",
      "Epoch 11/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.8514 - mean_absolute_error: 1.8514 - val_loss: 1.8746 - val_mean_absolute_error: 1.8746\n",
      "Epoch 12/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.8343 - mean_absolute_error: 1.8343 - val_loss: 1.8254 - val_mean_absolute_error: 1.8254\n",
      "Epoch 13/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.8191 - mean_absolute_error: 1.8191 - val_loss: 1.8081 - val_mean_absolute_error: 1.8081\n",
      "Epoch 14/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.8058 - mean_absolute_error: 1.8058 - val_loss: 1.8650 - val_mean_absolute_error: 1.8650\n",
      "Epoch 15/50\n",
      "567532/567532 [==============================] - 7s 11us/step - loss: 1.7944 - mean_absolute_error: 1.7944 - val_loss: 1.7319 - val_mean_absolute_error: 1.7319\n",
      "Epoch 16/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.7854 - mean_absolute_error: 1.7854 - val_loss: 1.7471 - val_mean_absolute_error: 1.7471\n",
      "Epoch 17/50\n",
      "567532/567532 [==============================] - 7s 12us/step - loss: 1.7758 - mean_absolute_error: 1.7758 - val_loss: 1.8189 - val_mean_absolute_error: 1.8189\n",
      "Epoch 18/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.7674 - mean_absolute_error: 1.7674 - val_loss: 1.8047 - val_mean_absolute_error: 1.8047\n",
      "Epoch 19/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.7604 - mean_absolute_error: 1.7604 - val_loss: 1.9995 - val_mean_absolute_error: 1.9995\n",
      "Epoch 20/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.7533 - mean_absolute_error: 1.7533 - val_loss: 1.7976 - val_mean_absolute_error: 1.7976\n",
      "Epoch 21/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.7467 - mean_absolute_error: 1.7467 - val_loss: 1.7809 - val_mean_absolute_error: 1.7809\n",
      "Epoch 22/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.7399 - mean_absolute_error: 1.7399 - val_loss: 1.7714 - val_mean_absolute_error: 1.7714\n",
      "Epoch 23/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.7344 - mean_absolute_error: 1.7344 - val_loss: 1.7184 - val_mean_absolute_error: 1.7184\n",
      "Epoch 24/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.7268 - mean_absolute_error: 1.7268 - val_loss: 1.7538 - val_mean_absolute_error: 1.7538\n",
      "Epoch 25/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.7199 - mean_absolute_error: 1.7199 - val_loss: 1.6791 - val_mean_absolute_error: 1.6791\n",
      "Epoch 26/50\n",
      "567532/567532 [==============================] - 7s 12us/step - loss: 1.7146 - mean_absolute_error: 1.7146 - val_loss: 1.7447 - val_mean_absolute_error: 1.7447\n",
      "Epoch 27/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.7094 - mean_absolute_error: 1.7094 - val_loss: 1.6881 - val_mean_absolute_error: 1.6881\n",
      "Epoch 28/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.7069 - mean_absolute_error: 1.7069 - val_loss: 1.6861 - val_mean_absolute_error: 1.6861\n",
      "Epoch 29/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.7022 - mean_absolute_error: 1.7022 - val_loss: 1.7648 - val_mean_absolute_error: 1.7648\n",
      "Epoch 30/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.6979 - mean_absolute_error: 1.6979 - val_loss: 1.7266 - val_mean_absolute_error: 1.7266\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567532/567532 [==============================] - 7s 12us/step - loss: 1.6939 - mean_absolute_error: 1.6939 - val_loss: 1.7493 - val_mean_absolute_error: 1.7493\n",
      "Epoch 32/50\n",
      "567532/567532 [==============================] - 7s 12us/step - loss: 1.6902 - mean_absolute_error: 1.6902 - val_loss: 1.7163 - val_mean_absolute_error: 1.7163\n",
      "Epoch 33/50\n",
      "567532/567532 [==============================] - 7s 11us/step - loss: 1.6874 - mean_absolute_error: 1.6874 - val_loss: 1.6844 - val_mean_absolute_error: 1.6844\n",
      "Epoch 34/50\n",
      "567532/567532 [==============================] - 7s 12us/step - loss: 1.6840 - mean_absolute_error: 1.6840 - val_loss: 1.6701 - val_mean_absolute_error: 1.6701\n",
      "Epoch 35/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.6811 - mean_absolute_error: 1.6811 - val_loss: 1.8204 - val_mean_absolute_error: 1.8204\n",
      "Epoch 36/50\n",
      "567532/567532 [==============================] - 7s 12us/step - loss: 1.6764 - mean_absolute_error: 1.6764 - val_loss: 1.7460 - val_mean_absolute_error: 1.7460\n",
      "Epoch 37/50\n",
      "567532/567532 [==============================] - 7s 12us/step - loss: 1.6730 - mean_absolute_error: 1.6730 - val_loss: 1.6407 - val_mean_absolute_error: 1.6407\n",
      "Epoch 38/50\n",
      "567532/567532 [==============================] - 7s 12us/step - loss: 1.6712 - mean_absolute_error: 1.6712 - val_loss: 1.6922 - val_mean_absolute_error: 1.6922\n",
      "Epoch 39/50\n",
      "567532/567532 [==============================] - 7s 11us/step - loss: 1.6674 - mean_absolute_error: 1.6674 - val_loss: 1.7274 - val_mean_absolute_error: 1.7274\n",
      "Epoch 40/50\n",
      "567532/567532 [==============================] - 7s 12us/step - loss: 1.6656 - mean_absolute_error: 1.6656 - val_loss: 1.6476 - val_mean_absolute_error: 1.6476\n",
      "Epoch 41/50\n",
      "567532/567532 [==============================] - 7s 12us/step - loss: 1.6621 - mean_absolute_error: 1.6621 - val_loss: 1.7963 - val_mean_absolute_error: 1.7963\n",
      "Epoch 42/50\n",
      "567532/567532 [==============================] - 7s 12us/step - loss: 1.6591 - mean_absolute_error: 1.6591 - val_loss: 1.6986 - val_mean_absolute_error: 1.6986\n",
      "Epoch 43/50\n",
      "567532/567532 [==============================] - 7s 12us/step - loss: 1.6540 - mean_absolute_error: 1.6540 - val_loss: 1.7131 - val_mean_absolute_error: 1.7131\n",
      "Epoch 44/50\n",
      "567532/567532 [==============================] - 7s 13us/step - loss: 1.6524 - mean_absolute_error: 1.6524 - val_loss: 1.5980 - val_mean_absolute_error: 1.5980\n",
      "Epoch 45/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.6504 - mean_absolute_error: 1.6504 - val_loss: 1.6112 - val_mean_absolute_error: 1.6112\n",
      "Epoch 46/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.6459 - mean_absolute_error: 1.6459 - val_loss: 1.8354 - val_mean_absolute_error: 1.8354\n",
      "Epoch 47/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.6436 - mean_absolute_error: 1.6436 - val_loss: 1.6705 - val_mean_absolute_error: 1.6705\n",
      "Epoch 48/50\n",
      "567532/567532 [==============================] - 7s 12us/step - loss: 1.6390 - mean_absolute_error: 1.6390 - val_loss: 1.6155 - val_mean_absolute_error: 1.6155\n",
      "Epoch 49/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.6374 - mean_absolute_error: 1.6374 - val_loss: 1.6633 - val_mean_absolute_error: 1.6633\n",
      "Epoch 50/50\n",
      "567532/567532 [==============================] - 6s 11us/step - loss: 1.6334 - mean_absolute_error: 1.6334 - val_loss: 1.5813 - val_mean_absolute_error: 1.5813\n",
      "1JHC Fold 0, logMAE: 0.45822176337242126\n",
      "(380609,)\n",
      "0.1527405877908071\n",
      "Train on 567533 samples, validate on 141883 samples\n",
      "Epoch 1/50\n",
      "567533/567533 [==============================] - 7s 12us/step - loss: 5.6323 - mean_absolute_error: 5.6323 - val_loss: 2.4462 - val_mean_absolute_error: 2.4462\n",
      "Epoch 2/50\n",
      "567533/567533 [==============================] - 7s 12us/step - loss: 2.3212 - mean_absolute_error: 2.3212 - val_loss: 2.3006 - val_mean_absolute_error: 2.3006\n",
      "Epoch 3/50\n",
      "567533/567533 [==============================] - 7s 13us/step - loss: 2.1895 - mean_absolute_error: 2.1895 - val_loss: 2.0555 - val_mean_absolute_error: 2.0555\n",
      "Epoch 4/50\n",
      "567533/567533 [==============================] - 7s 13us/step - loss: 2.1084 - mean_absolute_error: 2.1084 - val_loss: 2.0309 - val_mean_absolute_error: 2.0309\n",
      "Epoch 5/50\n",
      "567533/567533 [==============================] - 8s 13us/step - loss: 2.0487 - mean_absolute_error: 2.0487 - val_loss: 2.1183 - val_mean_absolute_error: 2.1183\n",
      "Epoch 6/50\n",
      "567533/567533 [==============================] - 7s 12us/step - loss: 2.0051 - mean_absolute_error: 2.0051 - val_loss: 2.0285 - val_mean_absolute_error: 2.0285\n",
      "Epoch 7/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.9703 - mean_absolute_error: 1.9703 - val_loss: 2.0075 - val_mean_absolute_error: 2.0075\n",
      "Epoch 8/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.9378 - mean_absolute_error: 1.9378 - val_loss: 1.8981 - val_mean_absolute_error: 1.8981\n",
      "Epoch 9/50\n",
      "567533/567533 [==============================] - 7s 12us/step - loss: 1.9100 - mean_absolute_error: 1.9100 - val_loss: 1.8928 - val_mean_absolute_error: 1.8928\n",
      "Epoch 10/50\n",
      "567533/567533 [==============================] - 7s 11us/step - loss: 1.8882 - mean_absolute_error: 1.8882 - val_loss: 1.8392 - val_mean_absolute_error: 1.8392\n",
      "Epoch 11/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.8668 - mean_absolute_error: 1.8668 - val_loss: 1.8359 - val_mean_absolute_error: 1.8359\n",
      "Epoch 12/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.8494 - mean_absolute_error: 1.8494 - val_loss: 1.8611 - val_mean_absolute_error: 1.8611\n",
      "Epoch 13/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.8358 - mean_absolute_error: 1.8358 - val_loss: 1.8755 - val_mean_absolute_error: 1.8755\n",
      "Epoch 14/50\n",
      "567533/567533 [==============================] - 7s 12us/step - loss: 1.8214 - mean_absolute_error: 1.8214 - val_loss: 1.7634 - val_mean_absolute_error: 1.7634\n",
      "Epoch 15/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.8103 - mean_absolute_error: 1.8103 - val_loss: 1.8486 - val_mean_absolute_error: 1.8486\n",
      "Epoch 16/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7995 - mean_absolute_error: 1.7995 - val_loss: 1.7818 - val_mean_absolute_error: 1.7818\n",
      "Epoch 17/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7899 - mean_absolute_error: 1.7899 - val_loss: 1.7888 - val_mean_absolute_error: 1.7888\n",
      "Epoch 18/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7821 - mean_absolute_error: 1.7821 - val_loss: 1.8596 - val_mean_absolute_error: 1.8596\n",
      "Epoch 19/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7730 - mean_absolute_error: 1.7730 - val_loss: 1.7271 - val_mean_absolute_error: 1.7271\n",
      "Epoch 20/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7647 - mean_absolute_error: 1.7647 - val_loss: 1.7337 - val_mean_absolute_error: 1.7337\n",
      "Epoch 21/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7596 - mean_absolute_error: 1.7596 - val_loss: 1.7687 - val_mean_absolute_error: 1.7687\n",
      "Epoch 22/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7515 - mean_absolute_error: 1.7515 - val_loss: 1.7708 - val_mean_absolute_error: 1.7708\n",
      "Epoch 23/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7454 - mean_absolute_error: 1.7454 - val_loss: 1.8414 - val_mean_absolute_error: 1.8414\n",
      "Epoch 24/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7398 - mean_absolute_error: 1.7398 - val_loss: 1.7064 - val_mean_absolute_error: 1.7064\n",
      "Epoch 25/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7347 - mean_absolute_error: 1.7347 - val_loss: 1.7232 - val_mean_absolute_error: 1.7232\n",
      "Epoch 26/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7289 - mean_absolute_error: 1.7289 - val_loss: 1.7961 - val_mean_absolute_error: 1.7961\n",
      "Epoch 27/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7252 - mean_absolute_error: 1.7252 - val_loss: 1.7627 - val_mean_absolute_error: 1.7627\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7205 - mean_absolute_error: 1.7205 - val_loss: 1.7045 - val_mean_absolute_error: 1.7045\n",
      "Epoch 29/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7137 - mean_absolute_error: 1.7137 - val_loss: 1.6702 - val_mean_absolute_error: 1.6702\n",
      "Epoch 30/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7080 - mean_absolute_error: 1.7080 - val_loss: 1.7156 - val_mean_absolute_error: 1.7156\n",
      "Epoch 31/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7022 - mean_absolute_error: 1.7022 - val_loss: 1.6988 - val_mean_absolute_error: 1.6988\n",
      "Epoch 32/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6979 - mean_absolute_error: 1.6979 - val_loss: 1.7776 - val_mean_absolute_error: 1.7776\n",
      "Epoch 33/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6909 - mean_absolute_error: 1.6909 - val_loss: 1.6305 - val_mean_absolute_error: 1.6305\n",
      "Epoch 34/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6854 - mean_absolute_error: 1.6854 - val_loss: 1.6853 - val_mean_absolute_error: 1.6853\n",
      "Epoch 35/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6825 - mean_absolute_error: 1.6825 - val_loss: 1.7170 - val_mean_absolute_error: 1.7170\n",
      "Epoch 36/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6792 - mean_absolute_error: 1.6792 - val_loss: 1.6165 - val_mean_absolute_error: 1.6165\n",
      "Epoch 37/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6730 - mean_absolute_error: 1.6730 - val_loss: 1.6810 - val_mean_absolute_error: 1.6810\n",
      "Epoch 38/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6696 - mean_absolute_error: 1.6696 - val_loss: 1.7974 - val_mean_absolute_error: 1.7974\n",
      "Epoch 39/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6677 - mean_absolute_error: 1.6677 - val_loss: 1.6514 - val_mean_absolute_error: 1.6514\n",
      "Epoch 40/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6610 - mean_absolute_error: 1.6610 - val_loss: 1.6601 - val_mean_absolute_error: 1.6601\n",
      "Epoch 41/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6596 - mean_absolute_error: 1.6596 - val_loss: 1.7701 - val_mean_absolute_error: 1.7701\n",
      "Epoch 42/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6547 - mean_absolute_error: 1.6547 - val_loss: 1.7360 - val_mean_absolute_error: 1.7360\n",
      "Epoch 43/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6509 - mean_absolute_error: 1.6509 - val_loss: 1.5991 - val_mean_absolute_error: 1.5991\n",
      "Epoch 44/50\n",
      "567533/567533 [==============================] - 7s 12us/step - loss: 1.6465 - mean_absolute_error: 1.6465 - val_loss: 1.6397 - val_mean_absolute_error: 1.6397\n",
      "Epoch 45/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6421 - mean_absolute_error: 1.6421 - val_loss: 1.6161 - val_mean_absolute_error: 1.6161\n",
      "Epoch 46/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6385 - mean_absolute_error: 1.6385 - val_loss: 1.6436 - val_mean_absolute_error: 1.6436\n",
      "Epoch 47/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6350 - mean_absolute_error: 1.6350 - val_loss: 1.7308 - val_mean_absolute_error: 1.7308\n",
      "Epoch 48/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6307 - mean_absolute_error: 1.6307 - val_loss: 1.6420 - val_mean_absolute_error: 1.6420\n",
      "Epoch 49/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6272 - mean_absolute_error: 1.6272 - val_loss: 1.6107 - val_mean_absolute_error: 1.6107\n",
      "Epoch 50/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6247 - mean_absolute_error: 1.6247 - val_loss: 1.5814 - val_mean_absolute_error: 1.5814\n",
      "1JHC Fold 1, logMAE: 0.45832502841949463\n",
      "(380609,)\n",
      "0.305515597263972\n",
      "Train on 567533 samples, validate on 141883 samples\n",
      "Epoch 1/50\n",
      "567533/567533 [==============================] - 7s 12us/step - loss: 5.7198 - mean_absolute_error: 5.7198 - val_loss: 2.3798 - val_mean_absolute_error: 2.3798\n",
      "Epoch 2/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 2.3009 - mean_absolute_error: 2.3009 - val_loss: 2.1721 - val_mean_absolute_error: 2.1721\n",
      "Epoch 3/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 2.1654 - mean_absolute_error: 2.1654 - val_loss: 2.0430 - val_mean_absolute_error: 2.0430\n",
      "Epoch 4/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 2.0800 - mean_absolute_error: 2.0800 - val_loss: 1.9990 - val_mean_absolute_error: 1.9990\n",
      "Epoch 5/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 2.0111 - mean_absolute_error: 2.0111 - val_loss: 1.9696 - val_mean_absolute_error: 1.9696\n",
      "Epoch 6/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.9639 - mean_absolute_error: 1.9639 - val_loss: 1.9496 - val_mean_absolute_error: 1.9496\n",
      "Epoch 7/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.9300 - mean_absolute_error: 1.9300 - val_loss: 1.9055 - val_mean_absolute_error: 1.9055\n",
      "Epoch 8/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.9022 - mean_absolute_error: 1.9022 - val_loss: 1.9304 - val_mean_absolute_error: 1.9304\n",
      "Epoch 9/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.8808 - mean_absolute_error: 1.8808 - val_loss: 1.8446 - val_mean_absolute_error: 1.8446\n",
      "Epoch 10/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.8630 - mean_absolute_error: 1.8630 - val_loss: 1.8434 - val_mean_absolute_error: 1.8434\n",
      "Epoch 11/50\n",
      "567533/567533 [==============================] - 7s 12us/step - loss: 1.8467 - mean_absolute_error: 1.8467 - val_loss: 1.8702 - val_mean_absolute_error: 1.8702\n",
      "Epoch 12/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.8327 - mean_absolute_error: 1.8327 - val_loss: 1.7835 - val_mean_absolute_error: 1.7835\n",
      "Epoch 13/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.8220 - mean_absolute_error: 1.8220 - val_loss: 1.7905 - val_mean_absolute_error: 1.7905\n",
      "Epoch 14/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.8101 - mean_absolute_error: 1.8101 - val_loss: 1.7705 - val_mean_absolute_error: 1.7705\n",
      "Epoch 15/50\n",
      "567533/567533 [==============================] - 7s 12us/step - loss: 1.7998 - mean_absolute_error: 1.7998 - val_loss: 1.7600 - val_mean_absolute_error: 1.7600\n",
      "Epoch 16/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7907 - mean_absolute_error: 1.7907 - val_loss: 1.7050 - val_mean_absolute_error: 1.7050\n",
      "Epoch 17/50\n",
      "567533/567533 [==============================] - 7s 12us/step - loss: 1.7825 - mean_absolute_error: 1.7825 - val_loss: 1.8892 - val_mean_absolute_error: 1.8892\n",
      "Epoch 18/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7722 - mean_absolute_error: 1.7722 - val_loss: 1.7562 - val_mean_absolute_error: 1.7562\n",
      "Epoch 19/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7666 - mean_absolute_error: 1.7666 - val_loss: 1.7188 - val_mean_absolute_error: 1.7188\n",
      "Epoch 20/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7588 - mean_absolute_error: 1.7588 - val_loss: 1.8643 - val_mean_absolute_error: 1.8643\n",
      "Epoch 21/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7503 - mean_absolute_error: 1.7503 - val_loss: 1.7961 - val_mean_absolute_error: 1.7961\n",
      "Epoch 22/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7426 - mean_absolute_error: 1.7426 - val_loss: 1.7628 - val_mean_absolute_error: 1.7628\n",
      "Epoch 23/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7366 - mean_absolute_error: 1.7366 - val_loss: 1.7685 - val_mean_absolute_error: 1.7685\n",
      "Epoch 24/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7302 - mean_absolute_error: 1.7302 - val_loss: 1.7677 - val_mean_absolute_error: 1.7677\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7252 - mean_absolute_error: 1.7252 - val_loss: 1.7770 - val_mean_absolute_error: 1.7770\n",
      "Epoch 26/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7193 - mean_absolute_error: 1.7193 - val_loss: 1.6783 - val_mean_absolute_error: 1.6783\n",
      "Epoch 27/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7139 - mean_absolute_error: 1.7139 - val_loss: 1.6723 - val_mean_absolute_error: 1.6723\n",
      "Epoch 28/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7090 - mean_absolute_error: 1.7090 - val_loss: 1.6404 - val_mean_absolute_error: 1.6404\n",
      "Epoch 29/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.7038 - mean_absolute_error: 1.7038 - val_loss: 1.8118 - val_mean_absolute_error: 1.8118\n",
      "Epoch 30/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6991 - mean_absolute_error: 1.6991 - val_loss: 1.7413 - val_mean_absolute_error: 1.7413\n",
      "Epoch 31/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6922 - mean_absolute_error: 1.6922 - val_loss: 1.7938 - val_mean_absolute_error: 1.7938\n",
      "Epoch 32/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6886 - mean_absolute_error: 1.6886 - val_loss: 1.6534 - val_mean_absolute_error: 1.6534\n",
      "Epoch 33/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6834 - mean_absolute_error: 1.6834 - val_loss: 1.7401 - val_mean_absolute_error: 1.7401\n",
      "Epoch 34/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6787 - mean_absolute_error: 1.6787 - val_loss: 1.7361 - val_mean_absolute_error: 1.7361\n",
      "Epoch 35/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6740 - mean_absolute_error: 1.6740 - val_loss: 1.8394 - val_mean_absolute_error: 1.8394\n",
      "Epoch 36/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6676 - mean_absolute_error: 1.6676 - val_loss: 1.8004 - val_mean_absolute_error: 1.8004\n",
      "Epoch 37/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6631 - mean_absolute_error: 1.6631 - val_loss: 1.6643 - val_mean_absolute_error: 1.6643\n",
      "Epoch 38/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6586 - mean_absolute_error: 1.6586 - val_loss: 1.6427 - val_mean_absolute_error: 1.6427\n",
      "Epoch 39/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6517 - mean_absolute_error: 1.6517 - val_loss: 1.6164 - val_mean_absolute_error: 1.6164\n",
      "Epoch 40/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6467 - mean_absolute_error: 1.6467 - val_loss: 1.7183 - val_mean_absolute_error: 1.7183\n",
      "Epoch 41/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6415 - mean_absolute_error: 1.6415 - val_loss: 1.6271 - val_mean_absolute_error: 1.6271\n",
      "Epoch 42/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6381 - mean_absolute_error: 1.6381 - val_loss: 1.6932 - val_mean_absolute_error: 1.6932\n",
      "Epoch 43/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6332 - mean_absolute_error: 1.6332 - val_loss: 1.6147 - val_mean_absolute_error: 1.6147\n",
      "Epoch 44/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6280 - mean_absolute_error: 1.6280 - val_loss: 1.6164 - val_mean_absolute_error: 1.6164\n",
      "Epoch 45/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6226 - mean_absolute_error: 1.6226 - val_loss: 1.8049 - val_mean_absolute_error: 1.8049\n",
      "Epoch 46/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6193 - mean_absolute_error: 1.6193 - val_loss: 1.5812 - val_mean_absolute_error: 1.5812\n",
      "Epoch 47/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6159 - mean_absolute_error: 1.6159 - val_loss: 1.6699 - val_mean_absolute_error: 1.6699\n",
      "Epoch 48/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6110 - mean_absolute_error: 1.6110 - val_loss: 1.7375 - val_mean_absolute_error: 1.7375\n",
      "Epoch 49/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6067 - mean_absolute_error: 1.6067 - val_loss: 1.5780 - val_mean_absolute_error: 1.5780\n",
      "Epoch 50/50\n",
      "567533/567533 [==============================] - 6s 11us/step - loss: 1.6033 - mean_absolute_error: 1.6033 - val_loss: 1.6545 - val_mean_absolute_error: 1.6545\n",
      "1JHC Fold 2, logMAE: 0.5035250186920166\n",
      "(380609,)\n",
      "0.47335727016131085\n",
      "*** Training Model for 2JHH ***\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1',\n",
      "       'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1',\n",
      "       'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1',\n",
      "       'd_8_2', 'd_8_3', 'scalar_coupling_constant'],\n",
      "      dtype='object')\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1',\n",
      "       'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1',\n",
      "       'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1',\n",
      "       'd_8_2', 'd_8_3'],\n",
      "      dtype='object')\n",
      "Train on 302428 samples, validate on 75608 samples\n",
      "Epoch 1/50\n",
      "302428/302428 [==============================] - 4s 12us/step - loss: 0.9181 - mean_absolute_error: 0.9181 - val_loss: 0.6189 - val_mean_absolute_error: 0.6189\n",
      "Epoch 2/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.5779 - mean_absolute_error: 0.5779 - val_loss: 0.5714 - val_mean_absolute_error: 0.5714\n",
      "Epoch 3/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.5256 - mean_absolute_error: 0.5256 - val_loss: 0.5304 - val_mean_absolute_error: 0.5304\n",
      "Epoch 4/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.4963 - mean_absolute_error: 0.4963 - val_loss: 0.4775 - val_mean_absolute_error: 0.4775\n",
      "Epoch 5/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.4755 - mean_absolute_error: 0.4755 - val_loss: 0.4541 - val_mean_absolute_error: 0.4541\n",
      "Epoch 6/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.4595 - mean_absolute_error: 0.4595 - val_loss: 0.4965 - val_mean_absolute_error: 0.4965\n",
      "Epoch 7/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.4466 - mean_absolute_error: 0.4466 - val_loss: 0.4466 - val_mean_absolute_error: 0.4466\n",
      "Epoch 8/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.4362 - mean_absolute_error: 0.4362 - val_loss: 0.4293 - val_mean_absolute_error: 0.4293\n",
      "Epoch 9/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.4278 - mean_absolute_error: 0.4278 - val_loss: 0.4391 - val_mean_absolute_error: 0.4391\n",
      "Epoch 10/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.4210 - mean_absolute_error: 0.4210 - val_loss: 0.4294 - val_mean_absolute_error: 0.4294\n",
      "Epoch 11/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.4151 - mean_absolute_error: 0.4151 - val_loss: 0.4300 - val_mean_absolute_error: 0.4300\n",
      "Epoch 12/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.4094 - mean_absolute_error: 0.4094 - val_loss: 0.4003 - val_mean_absolute_error: 0.4003\n",
      "Epoch 13/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.4044 - mean_absolute_error: 0.4044 - val_loss: 0.3973 - val_mean_absolute_error: 0.3973\n",
      "Epoch 14/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.4004 - mean_absolute_error: 0.4004 - val_loss: 0.3929 - val_mean_absolute_error: 0.3929\n",
      "Epoch 15/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3961 - mean_absolute_error: 0.3961 - val_loss: 0.4003 - val_mean_absolute_error: 0.4003\n",
      "Epoch 16/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3921 - mean_absolute_error: 0.3921 - val_loss: 0.3821 - val_mean_absolute_error: 0.3821\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3878 - mean_absolute_error: 0.3878 - val_loss: 0.3818 - val_mean_absolute_error: 0.3818\n",
      "Epoch 18/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3845 - mean_absolute_error: 0.3845 - val_loss: 0.3805 - val_mean_absolute_error: 0.3805\n",
      "Epoch 19/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3819 - mean_absolute_error: 0.3819 - val_loss: 0.3985 - val_mean_absolute_error: 0.3985\n",
      "Epoch 20/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3786 - mean_absolute_error: 0.3786 - val_loss: 0.3941 - val_mean_absolute_error: 0.3941\n",
      "Epoch 21/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3757 - mean_absolute_error: 0.3757 - val_loss: 0.3788 - val_mean_absolute_error: 0.3788\n",
      "Epoch 22/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3732 - mean_absolute_error: 0.3732 - val_loss: 0.3746 - val_mean_absolute_error: 0.3746\n",
      "Epoch 23/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3710 - mean_absolute_error: 0.3710 - val_loss: 0.3722 - val_mean_absolute_error: 0.3722\n",
      "Epoch 24/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3685 - mean_absolute_error: 0.3685 - val_loss: 0.4070 - val_mean_absolute_error: 0.4070\n",
      "Epoch 25/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3663 - mean_absolute_error: 0.3663 - val_loss: 0.3687 - val_mean_absolute_error: 0.3687\n",
      "Epoch 26/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3646 - mean_absolute_error: 0.3646 - val_loss: 0.3654 - val_mean_absolute_error: 0.3654\n",
      "Epoch 27/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3621 - mean_absolute_error: 0.3621 - val_loss: 0.3848 - val_mean_absolute_error: 0.3848\n",
      "Epoch 28/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3609 - mean_absolute_error: 0.3609 - val_loss: 0.3541 - val_mean_absolute_error: 0.3541\n",
      "Epoch 29/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3590 - mean_absolute_error: 0.3590 - val_loss: 0.3601 - val_mean_absolute_error: 0.3601\n",
      "Epoch 30/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3574 - mean_absolute_error: 0.3574 - val_loss: 0.4057 - val_mean_absolute_error: 0.4057\n",
      "Epoch 31/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3559 - mean_absolute_error: 0.3559 - val_loss: 0.3540 - val_mean_absolute_error: 0.3540\n",
      "Epoch 32/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3544 - mean_absolute_error: 0.3544 - val_loss: 0.3797 - val_mean_absolute_error: 0.3797\n",
      "Epoch 33/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3528 - mean_absolute_error: 0.3528 - val_loss: 0.3611 - val_mean_absolute_error: 0.3611\n",
      "Epoch 34/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3521 - mean_absolute_error: 0.3521 - val_loss: 0.3479 - val_mean_absolute_error: 0.3479\n",
      "Epoch 35/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3507 - mean_absolute_error: 0.3507 - val_loss: 0.3623 - val_mean_absolute_error: 0.3623\n",
      "Epoch 36/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3490 - mean_absolute_error: 0.3490 - val_loss: 0.3480 - val_mean_absolute_error: 0.3480\n",
      "Epoch 37/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3478 - mean_absolute_error: 0.3478 - val_loss: 0.3536 - val_mean_absolute_error: 0.3536\n",
      "Epoch 38/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3473 - mean_absolute_error: 0.3473 - val_loss: 0.3657 - val_mean_absolute_error: 0.3657\n",
      "Epoch 39/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3452 - mean_absolute_error: 0.3452 - val_loss: 0.3470 - val_mean_absolute_error: 0.3470\n",
      "Epoch 40/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3441 - mean_absolute_error: 0.3441 - val_loss: 0.3462 - val_mean_absolute_error: 0.3462\n",
      "Epoch 41/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3432 - mean_absolute_error: 0.3432 - val_loss: 0.3469 - val_mean_absolute_error: 0.3469\n",
      "Epoch 42/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3421 - mean_absolute_error: 0.3421 - val_loss: 0.3705 - val_mean_absolute_error: 0.3705\n",
      "Epoch 43/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3413 - mean_absolute_error: 0.3413 - val_loss: 0.3491 - val_mean_absolute_error: 0.3491\n",
      "Epoch 44/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3399 - mean_absolute_error: 0.3399 - val_loss: 0.3436 - val_mean_absolute_error: 0.3436\n",
      "Epoch 45/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3391 - mean_absolute_error: 0.3391 - val_loss: 0.3357 - val_mean_absolute_error: 0.3357\n",
      "Epoch 46/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3378 - mean_absolute_error: 0.3378 - val_loss: 0.3419 - val_mean_absolute_error: 0.3419\n",
      "Epoch 47/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3370 - mean_absolute_error: 0.3370 - val_loss: 0.3274 - val_mean_absolute_error: 0.3274\n",
      "Epoch 48/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3358 - mean_absolute_error: 0.3358 - val_loss: 0.3660 - val_mean_absolute_error: 0.3660\n",
      "Epoch 49/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3349 - mean_absolute_error: 0.3349 - val_loss: 0.3593 - val_mean_absolute_error: 0.3593\n",
      "Epoch 50/50\n",
      "302428/302428 [==============================] - 3s 11us/step - loss: 0.3336 - mean_absolute_error: 0.3336 - val_loss: 0.3365 - val_mean_absolute_error: 0.3365\n",
      "2JHH Fold 0, logMAE: -1.0892146825790405\n",
      "(203126,)\n",
      "-0.3630715608596802\n",
      "Train on 302429 samples, validate on 75607 samples\n",
      "Epoch 1/50\n",
      "302429/302429 [==============================] - 4s 14us/step - loss: 0.9182 - mean_absolute_error: 0.9182 - val_loss: 0.6285 - val_mean_absolute_error: 0.6285\n",
      "Epoch 2/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.5981 - mean_absolute_error: 0.5981 - val_loss: 0.6370 - val_mean_absolute_error: 0.6370\n",
      "Epoch 3/50\n",
      "302429/302429 [==============================] - 4s 12us/step - loss: 0.5533 - mean_absolute_error: 0.5533 - val_loss: 0.5537 - val_mean_absolute_error: 0.5537\n",
      "Epoch 4/50\n",
      "302429/302429 [==============================] - 4s 12us/step - loss: 0.5259 - mean_absolute_error: 0.5259 - val_loss: 0.5327 - val_mean_absolute_error: 0.5327\n",
      "Epoch 5/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.5035 - mean_absolute_error: 0.5035 - val_loss: 0.5001 - val_mean_absolute_error: 0.5001\n",
      "Epoch 6/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.4857 - mean_absolute_error: 0.4857 - val_loss: 0.4674 - val_mean_absolute_error: 0.4674\n",
      "Epoch 7/50\n",
      "302429/302429 [==============================] - 4s 12us/step - loss: 0.4701 - mean_absolute_error: 0.4701 - val_loss: 0.5240 - val_mean_absolute_error: 0.5240\n",
      "Epoch 8/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.4579 - mean_absolute_error: 0.4579 - val_loss: 0.4432 - val_mean_absolute_error: 0.4432\n",
      "Epoch 9/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.4467 - mean_absolute_error: 0.4467 - val_loss: 0.4444 - val_mean_absolute_error: 0.4444\n",
      "Epoch 10/50\n",
      "302429/302429 [==============================] - 4s 12us/step - loss: 0.4377 - mean_absolute_error: 0.4377 - val_loss: 0.4123 - val_mean_absolute_error: 0.4123\n",
      "Epoch 11/50\n",
      "302429/302429 [==============================] - 4s 12us/step - loss: 0.4294 - mean_absolute_error: 0.4294 - val_loss: 0.4857 - val_mean_absolute_error: 0.4857\n",
      "Epoch 12/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.4233 - mean_absolute_error: 0.4233 - val_loss: 0.3977 - val_mean_absolute_error: 0.3977\n",
      "Epoch 13/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.4169 - mean_absolute_error: 0.4169 - val_loss: 0.4113 - val_mean_absolute_error: 0.4113\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.4114 - mean_absolute_error: 0.4114 - val_loss: 0.4201 - val_mean_absolute_error: 0.4201\n",
      "Epoch 15/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.4063 - mean_absolute_error: 0.4063 - val_loss: 0.3975 - val_mean_absolute_error: 0.3975\n",
      "Epoch 16/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.4014 - mean_absolute_error: 0.4014 - val_loss: 0.4105 - val_mean_absolute_error: 0.4105\n",
      "Epoch 17/50\n",
      "302429/302429 [==============================] - 4s 12us/step - loss: 0.3969 - mean_absolute_error: 0.3969 - val_loss: 0.3948 - val_mean_absolute_error: 0.3948\n",
      "Epoch 18/50\n",
      "302429/302429 [==============================] - 4s 12us/step - loss: 0.3936 - mean_absolute_error: 0.3936 - val_loss: 0.4017 - val_mean_absolute_error: 0.4017\n",
      "Epoch 19/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.3896 - mean_absolute_error: 0.3896 - val_loss: 0.3975 - val_mean_absolute_error: 0.3975\n",
      "Epoch 20/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.3869 - mean_absolute_error: 0.3869 - val_loss: 0.3835 - val_mean_absolute_error: 0.3835\n",
      "Epoch 21/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.3839 - mean_absolute_error: 0.3839 - val_loss: 0.4382 - val_mean_absolute_error: 0.4382\n",
      "Epoch 22/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.3808 - mean_absolute_error: 0.3808 - val_loss: 0.3728 - val_mean_absolute_error: 0.3728\n",
      "Epoch 23/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.3777 - mean_absolute_error: 0.3777 - val_loss: 0.3641 - val_mean_absolute_error: 0.3641\n",
      "Epoch 24/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.3755 - mean_absolute_error: 0.3755 - val_loss: 0.3745 - val_mean_absolute_error: 0.3745\n",
      "Epoch 25/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.3724 - mean_absolute_error: 0.3724 - val_loss: 0.3829 - val_mean_absolute_error: 0.3829\n",
      "Epoch 26/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.3699 - mean_absolute_error: 0.3699 - val_loss: 0.4132 - val_mean_absolute_error: 0.4132\n",
      "Epoch 27/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.3684 - mean_absolute_error: 0.3684 - val_loss: 0.4092 - val_mean_absolute_error: 0.4092\n",
      "Epoch 28/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.3663 - mean_absolute_error: 0.3663 - val_loss: 0.3784 - val_mean_absolute_error: 0.3784\n",
      "Epoch 29/50\n",
      "302429/302429 [==============================] - 3s 11us/step - loss: 0.3640 - mean_absolute_error: 0.3640 - val_loss: 0.3811 - val_mean_absolute_error: 0.3811\n",
      "Epoch 30/50\n",
      "243456/302429 [=======================>......] - ETA: 0s - loss: 0.3639 - mean_absolute_error: 0.3639"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    '1JHN': 7,\n",
    "    '1JHC': 10,\n",
    "    '2JHH': 9,\n",
    "    '2JHN': 9,\n",
    "    '2JHC': 9,\n",
    "    '3JHH': 9,\n",
    "    '3JHC': 10,\n",
    "    '3JHN': 10\n",
    "}\n",
    "N_FOLDS = 3\n",
    "submission = submission_csv.copy()\n",
    "\n",
    "cv_scores = {}\n",
    "for coupling_type in model_params.keys():\n",
    "    cv_score = train_and_predict_for_one_coupling_type(\n",
    "        coupling_type, submission, n_atoms=model_params[coupling_type], n_folds=N_FOLDS)\n",
    "    cv_scores[coupling_type] = cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>cv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1JHN</td>\n",
       "      <td>-0.989828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1JHC</td>\n",
       "      <td>-0.259521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2JHH</td>\n",
       "      <td>-1.739042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2JHN</td>\n",
       "      <td>-1.957707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2JHC</td>\n",
       "      <td>-1.182418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3JHH</td>\n",
       "      <td>-1.743725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3JHC</td>\n",
       "      <td>-1.093818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3JHN</td>\n",
       "      <td>-2.196844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  cv_score\n",
       "0  1JHN -0.989828\n",
       "1  1JHC -0.259521\n",
       "2  2JHH -1.739042\n",
       "3  2JHN -1.957707\n",
       "4  2JHC -1.182418\n",
       "5  3JHH -1.743725\n",
       "6  3JHC -1.093818\n",
       "7  3JHN -2.196844"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'type': list(cv_scores.keys()), 'cv_score': list(cv_scores.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3953628002312313"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(cv_scores.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[submission['scalar_coupling_constant'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4658147</th>\n",
       "      <td>24.443258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658148</th>\n",
       "      <td>142.937302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658149</th>\n",
       "      <td>9.953008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658150</th>\n",
       "      <td>142.937302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658151</th>\n",
       "      <td>24.443258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658152</th>\n",
       "      <td>93.242844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658153</th>\n",
       "      <td>2.555109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658154</th>\n",
       "      <td>-7.562157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658155</th>\n",
       "      <td>-9.632586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658156</th>\n",
       "      <td>93.242844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         scalar_coupling_constant\n",
       "id                               \n",
       "4658147  24.443258               \n",
       "4658148  142.937302              \n",
       "4658149  9.953008                \n",
       "4658150  142.937302              \n",
       "4658151  24.443258               \n",
       "4658152  93.242844               \n",
       "4658153  2.555109                \n",
       "4658154 -7.562157                \n",
       "4658155 -9.632586                \n",
       "4658156  93.242844               "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(f'{SUBMISSIONS_PATH}/sub_lightgbm_two_params.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = list(df.columns)\n",
    "cols.remove('scalar_coupling_constant')\n",
    "cols\n",
    "df_importance = pd.DataFrame({'feature': cols, 'importance': model.feature_importances_})\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=df_importance.sort_values('importance', ascending=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
