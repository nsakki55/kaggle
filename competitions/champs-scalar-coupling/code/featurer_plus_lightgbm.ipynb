{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/marcelotamashiro/lgb-public-kernels-plus-more-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../input/structures_yukawa.csv' does not exist: b'../input/structures_yukawa.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4641c870dc24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/structures.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m \u001b[0mstructures_yukawa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/structures_yukawa.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructures_yukawa\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mstructures_yukawa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpu-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpu-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpu-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpu-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpu-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../input/structures_yukawa.csv' does not exist: b'../input/structures_yukawa.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection._split import check_cv\n",
    "from sklearn.base import clone, is_classifier\n",
    "from scipy.stats import kurtosis, skew\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "class ClassifierTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator=None, n_classes=2, cv=3):\n",
    "        self.estimator = estimator\n",
    "        self.n_classes = n_classes\n",
    "        self.cv = cv\n",
    "    \n",
    "    def _get_labels(self, y):\n",
    "        y_labels = np.zeros(len(y))\n",
    "        y_us = np.sort(np.unique(y))\n",
    "        step = int(len(y_us) / self.n_classes)\n",
    "        \n",
    "        for i_class in range(self.n_classes):\n",
    "            if i_class + 1 == self.n_classes:\n",
    "                y_labels[y >= y_us[i_class * step]] = i_class\n",
    "            else:\n",
    "                y_labels[\n",
    "                    np.logical_and(\n",
    "                        y >= y_us[i_class * step],\n",
    "                        y < y_us[(i_class + 1) * step]\n",
    "                    )\n",
    "                ] = i_class\n",
    "        return y_labels\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = X.replace([np.inf,-np.inf], np.nan)\n",
    "        X = X.fillna(0)\n",
    "        y_labels = self._get_labels(y)\n",
    "        cv = check_cv(self.cv, y_labels, classifier=is_classifier(self.estimator))\n",
    "        self.estimators_ = []\n",
    "        \n",
    "        for train, _ in cv.split(X, y_labels):\n",
    "            X = np.array(X)\n",
    "            self.estimators_.append(\n",
    "                clone(self.estimator).fit(X[train], y_labels[train])\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        cv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))\n",
    "        X = X.replace([np.inf,-np.inf], np.nan)\n",
    "        X = X.fillna(0)\n",
    "        X = np.array(X)\n",
    "        X_prob = np.zeros((X.shape[0], self.n_classes))\n",
    "        X_pred = np.zeros(X.shape[0])\n",
    "        \n",
    "        for estimator, (_, test) in zip(self.estimators_, cv.split(X)):\n",
    "            X_prob[test] = estimator.predict_proba(X[test])\n",
    "            X_pred[test] = estimator.predict(X[test])\n",
    "        return np.hstack([X_prob, np.array([X_pred]).T])\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "all_features = ['type',  'atom_x', 'x_x', 'y_x','z_x', 'n_bonds_x', 'atom_y', 'x_y', 'y_y',\n",
    "       'z_y', 'n_bonds_y', 'C', 'F', 'H', 'N', 'O', 'distance', 'dist_mean_x','dist_mean_y',\n",
    "       'x_dist', 'y_dist', 'z_dist', 'x_dist_abs', 'y_dist_abs', 'z_dist_abs','inv_distance3']\n",
    "cat_features = ['type','atom_x','atom_y']\n",
    "\n",
    "class MoreStructureProperties(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self,atomic_radius,electronegativity):\n",
    "        self.atomic_radius = atomic_radius\n",
    "        self.electronegativity = electronegativity\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        atom_rad = [self.atomic_radius[x] for x in X['atom'].values]\n",
    "        X['rad'] = atom_rad\n",
    "        position = X[['x','y','z']].values\n",
    "        p_temp = position\n",
    "        molec_name = X['molecule_name'].values\n",
    "        m_temp = molec_name\n",
    "        radius = X['rad'].values\n",
    "        r_temp = radius\n",
    "        bond = 0\n",
    "        dist_keep = 0\n",
    "        dist_bond = 0 \n",
    "        no_bond = 0\n",
    "        dist_no_bond = 0\n",
    "        dist_matrix = np.zeros((X.shape[0],2*29))\n",
    "        dist_matrix_bond = np.zeros((X.shape[0],2*29))\n",
    "        dist_matrix_no_bond = np.zeros((X.shape[0],2*29))\n",
    "        \n",
    "        for i in range(29):\n",
    "            p_temp = np.roll(p_temp,-1,axis=0)\n",
    "            m_temp = np.roll(m_temp,-1,axis=0)\n",
    "            r_temp = np.roll(r_temp,-1,axis=0)\n",
    "            mask = (m_temp==molec_name)\n",
    "            dist = np.linalg.norm(position-p_temp,axis=1) * mask            \n",
    "            dist_temp = np.roll(np.linalg.norm(position-p_temp,axis=1)*mask,i+1,axis=0)\n",
    "            diff_radius_dist = (dist-(radius+r_temp)) * (dist<(radius+r_temp)) * mask\n",
    "            diff_radius_dist_temp = np.roll(diff_radius_dist,i+1,axis=0)\n",
    "            bond += (dist<(radius+r_temp)) * mask\n",
    "            bond_temp = np.roll((dist<(radius+r_temp)) * mask,i+1,axis=0)\n",
    "            no_bond += (dist>=(radius+r_temp)) * mask\n",
    "            no_bond_temp = np.roll((dist>=(radius+r_temp)) * mask,i+1,axis=0)\n",
    "            bond += bond_temp\n",
    "            no_bond += no_bond_temp\n",
    "            dist_keep += dist * mask\n",
    "            dist_matrix[:,2*i] = dist\n",
    "            dist_matrix[:,2*i+1] = dist_temp\n",
    "            dist_matrix_bond[:,2*i] = dist * (dist<(radius+r_temp)) * mask\n",
    "            dist_matrix_bond[:,2*i+1] = dist_temp * bond_temp\n",
    "            dist_matrix_no_bond[:,2*i] = dist * (dist>(radius+r_temp)) * mask\n",
    "            dist_matrix_no_bond[:,2*i+1] = dist_temp * no_bond_temp\n",
    "        X['n_bonds'] = bond\n",
    "        X['n_no_bonds'] = no_bond\n",
    "        X['dist_mean'] = np.nanmean(np.where(dist_matrix==0,np.nan,dist_matrix), axis=1)\n",
    "        X['dist_median'] = np.nanmedian(np.where(dist_matrix==0,np.nan,dist_matrix), axis=1)\n",
    "        X['dist_std_bond'] = np.nanstd(np.where(dist_matrix_bond==0,np.nan,dist_matrix), axis=1)\n",
    "        X['dist_mean_bond'] = np.nanmean(np.where(dist_matrix_bond==0,np.nan,dist_matrix), axis=1)\n",
    "        X['dist_median_bond'] = np.nanmedian(np.where(dist_matrix_bond==0,np.nan,dist_matrix), axis=1)\n",
    "        X['dist_mean_no_bond'] = np.nanmean(np.where(dist_matrix_no_bond==0,np.nan,dist_matrix), axis=1)\n",
    "        X['dist_std_no_bond'] = np.nanstd(np.where(dist_matrix_no_bond==0,np.nan,dist_matrix), axis=1)\n",
    "        X['dist_median_no_bond'] = np.nanmedian(np.where(dist_matrix_no_bond==0,np.nan,dist_matrix), axis=1)\n",
    "        X['dist_std'] = np.nanstd(np.where(dist_matrix==0,np.nan,dist_matrix), axis=1)\n",
    "        X['dist_min'] = np.nanmin(np.where(dist_matrix==0,np.nan,dist_matrix), axis=1)\n",
    "        X['dist_max'] = np.nanmax(np.where(dist_matrix==0,np.nan,dist_matrix), axis=1)\n",
    "        X['range_dist'] = np.absolute(X['dist_max']-X['dist_min'])\n",
    "        X['dist_bond_min'] = np.nanmin(np.where(dist_matrix_bond==0,np.nan,dist_matrix), axis=1)\n",
    "        X['dist_bond_max'] = np.nanmax(np.where(dist_matrix_bond==0,np.nan,dist_matrix), axis=1)\n",
    "        X['range_dist_bond'] = np.absolute(X['dist_bond_max']-X['dist_bond_min'])\n",
    "        X['dist_no_bond_min'] = np.nanmin(np.where(dist_matrix_no_bond==0,np.nan,dist_matrix), axis=1)\n",
    "        X['dist_no_bond_max'] = np.nanmax(np.where(dist_matrix_no_bond==0,np.nan,dist_matrix), axis=1)\n",
    "        X['range_dist_no_bond'] = np.absolute(X['dist_no_bond_max']-X['dist_no_bond_min'])\n",
    "        X['n_diff'] = pd.DataFrame(np.around(dist_matrix_bond,5)).nunique(axis=1).values  #5\n",
    "        X = reduce_mem_usage(X,verbose=False)\n",
    "        return X\n",
    "        \n",
    "    \n",
    "class MakeMoreFeatures(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['distance'] = np.linalg.norm(X[['x_x','y_x','z_x']].values - X[['x_y','y_y','z_y']].values ,axis=1)\n",
    "        X['x_dist'] = X['x_x'] - X['x_y']\n",
    "        X['y_dist'] = X['y_x'] - X['y_y']\n",
    "        X['z_dist'] = X['z_x'] - X['z_y']\n",
    "        X['x_dist_abs'] = np.absolute(X['x_dist'])\n",
    "        X['y_dist_abs'] = np.absolute(X['y_dist'])\n",
    "        X['z_dist_abs'] = np.absolute(X['z_dist'])\n",
    "        X['inv_distance3'] = 1/(X['distance']**3)\n",
    "        X['dimension_x'] = np.absolute(X.groupby(['molecule_name'])['x_x'].transform('max') - X.groupby(['molecule_name'])['x_x'].transform('min'))\n",
    "        X['dimension_y'] = np.absolute(X.groupby(['molecule_name'])['y_x'].transform('max') - X.groupby(['molecule_name'])['y_x'].transform('min'))\n",
    "        X['dimension_z'] = np.absolute(X.groupby(['molecule_name'])['z_x'].transform('max') - X.groupby(['molecule_name'])['z_x'].transform('min'))\n",
    "        X['molecule_dist_mean_x'] = X.groupby(['molecule_name'])['dist_mean_x'].transform('mean')\n",
    "        X['molecule_dist_mean_y'] = X.groupby(['molecule_name'])['dist_mean_y'].transform('mean')\n",
    "        X['molecule_dist_mean_bond_x'] = X.groupby(['molecule_name'])['dist_mean_bond_x'].transform('mean')\n",
    "        X['molecule_dist_mean_bond_y'] = X.groupby(['molecule_name'])['dist_mean_bond_y'].transform('mean')\n",
    "        X['molecule_dist_range_x'] = X.groupby(['molecule_name'])['dist_mean_x'].transform('max') - X.groupby(['molecule_name'])['dist_mean_x'].transform('min')\n",
    "        X['molecule_dist_range_y'] = X.groupby(['molecule_name'])['dist_mean_y'].transform('max') - X.groupby(['molecule_name'])['dist_mean_y'].transform('min')\n",
    "        X['molecule_dist_std_x'] = X.groupby(['molecule_name'])['dist_mean_x'].transform('std')\n",
    "        X['molecule_dist_std_y'] = X.groupby(['molecule_name'])['dist_mean_y'].transform('std')\n",
    "        X['molecule_atom_0_dist_mean'] = X.groupby(['molecule_name','atom_x'])['distance'].transform('mean')\n",
    "        X['molecule_atom_1_dist_mean'] = X.groupby(['molecule_name','atom_y'])['distance'].transform('mean')\n",
    "        X['molecule_atom_0_dist_std_diff'] = X.groupby(['molecule_name', 'atom_x'])['distance'].transform('std') - X['distance']\n",
    "        X['molecule_atom_1_dist_std_diff'] = X.groupby(['molecule_name', 'atom_y'])['distance'].transform('std') - X['distance']\n",
    "        X['molecule_type_dist_min'] = X.groupby(['molecule_name','type'])['distance'].transform('min') \n",
    "        X['molecule_type_dist_max'] = X.groupby(['molecule_name','type'])['distance'].transform('max') \n",
    "        X['molecule_dist_mean_no_bond_x'] = X.groupby(['molecule_name'])['dist_mean_no_bond_x'].transform('mean')\n",
    "        X['molecule_dist_mean_no_bond_y'] = X.groupby(['molecule_name'])['dist_mean_no_bond_y'].transform('mean')\n",
    "        X['molecule_atom_index_0_dist_min'] = X.groupby(['molecule_name', 'atom_index_0'])['distance'].transform('min') #new variable - dont include\n",
    "        X['molecule_atom_index_0_dist_std'] = X.groupby(['molecule_name', 'atom_index_0'])['distance'].transform('std') #new variable - dont include\n",
    "        X['molecule_atom_index_0_dist_min_div'] = X['molecule_atom_index_0_dist_min']/X['distance'] #new variable - include\n",
    "        X['molecule_atom_index_0_dist_std_div'] = X['molecule_atom_index_0_dist_std']/X['distance'] #new variable - include\n",
    "        X['molecule_atom_index_0_dist_mean'] = X.groupby(['molecule_name', 'atom_index_0'])['distance'].transform('mean') #new variable - include\n",
    "        X['molecule_atom_index_0_dist_max'] = X.groupby(['molecule_name', 'atom_index_0'])['distance'].transform('max') #new variable - include\n",
    "        X['molecule_atom_index_0_dist_mean_diff'] = X['molecule_atom_index_0_dist_mean'] - X['distance'] #new variable - include\n",
    "        X['molecule_atom_index_1_dist_mean'] = X.groupby(['molecule_name', 'atom_index_1'])['distance'].transform('mean') #new variable - include\n",
    "        X['molecule_atom_index_1_dist_max'] = X.groupby(['molecule_name', 'atom_index_1'])['distance'].transform('max') #new variable - include\n",
    "        X['molecule_atom_index_1_dist_min'] = X.groupby(['molecule_name', 'atom_index_1'])['distance'].transform('min') #new variable - include\n",
    "        X['molecule_atom_index_1_dist_std'] = X.groupby(['molecule_name', 'atom_index_1'])['distance'].transform('std') #new variable - dont include\n",
    "        X['molecule_atom_index_1_dist_min_div'] = X['molecule_atom_index_1_dist_min']/X['distance'] #new variable - include\n",
    "        X['molecule_atom_index_1_dist_std_diff'] = X['molecule_atom_index_1_dist_std'] - X['distance'] #new variable - include\n",
    "        X['molecule_atom_index_1_dist_mean_div'] = X['molecule_atom_index_1_dist_mean']/X['distance'] #new variable - include\n",
    "        X['molecule_atom_index_1_dist_min_diff'] = X['molecule_atom_index_1_dist_min_div'] - X['distance'] #new variable - include\n",
    "        le = LabelEncoder()\n",
    "        for feat in ['atom_x','atom_y']:\n",
    "            le.fit(X[feat])\n",
    "            X[feat] = le.transform(X[feat])\n",
    "        X = reduce_mem_usage(X,verbose=False)\n",
    "        return X\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "\n",
    "def map_atom_info(df_1, df_2, atom_idx):\n",
    "    df = pd.merge(df_1, df_2, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    \n",
    "    df = df.drop('atom_index', axis=1)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def find_dist(df):\n",
    "    df_p_0 = df[['x_0', 'y_0', 'z_0']].values\n",
    "    df_p_1 = df[['x_1', 'y_1', 'z_1']].values\n",
    "    \n",
    "    df['dist'] = np.linalg.norm(df_p_0 - df_p_1, axis=1)\n",
    "    df['dist_inv2'] = 1/df['dist']**2\n",
    "    df['dist_x'] = (df['x_0'] - df['x_1']) ** 2\n",
    "    df['dist_y'] = (df['y_0'] - df['y_1']) ** 2\n",
    "    df['dist_z'] = (df['z_0'] - df['z_1']) ** 2\n",
    "    return df\n",
    "\n",
    "def find_closest_atom(df):    \n",
    "    df_temp = df.loc[:,[\"molecule_name\",\n",
    "                      \"atom_index_0\",\"atom_index_1\",\n",
    "                      \"dist\",\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"]].copy()\n",
    "    df_temp_ = df_temp.copy()\n",
    "    df_temp_ = df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n",
    "                                       'atom_index_1': 'atom_index_0',\n",
    "                                       'x_0': 'x_1',\n",
    "                                       'y_0': 'y_1',\n",
    "                                       'z_0': 'z_1',\n",
    "                                       'x_1': 'x_0',\n",
    "                                       'y_1': 'y_0',\n",
    "                                       'z_1': 'z_0'})\n",
    "    df_temp_all = pd.concat((df_temp,df_temp_),axis=0)\n",
    "\n",
    "    df_temp_all[\"min_distance\"]=df_temp_all.groupby(['molecule_name', \n",
    "                                                     'atom_index_0'])['dist'].transform('min')\n",
    "    df_temp_all[\"max_distance\"]=df_temp_all.groupby(['molecule_name', \n",
    "                                                     'atom_index_0'])['dist'].transform('max')\n",
    "    \n",
    "    df_temp = df_temp_all[df_temp_all[\"min_distance\"]==df_temp_all[\"dist\"]].copy()\n",
    "    df_temp = df_temp.drop(['x_0','y_0','z_0','min_distance'], axis=1)\n",
    "    df_temp = df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                         'atom_index_1': 'atom_index_closest',\n",
    "                                         'dist': 'distance_closest',\n",
    "                                         'x_1': 'x_closest',\n",
    "                                         'y_1': 'y_closest',\n",
    "                                         'z_1': 'z_closest'})\n",
    "    df_temp = df_temp.drop_duplicates(subset=['molecule_name', 'atom_index'])\n",
    "    \n",
    "    for atom_idx in [0,1]:\n",
    "        df = map_atom_info(df,df_temp, atom_idx)\n",
    "        df = df.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n",
    "                                        'distance_closest': f'distance_closest_{atom_idx}',\n",
    "                                        'x_closest': f'x_closest_{atom_idx}',\n",
    "                                        'y_closest': f'y_closest_{atom_idx}',\n",
    "                                        'z_closest': f'z_closest_{atom_idx}'})\n",
    "        \n",
    "    df_temp= df_temp_all[df_temp_all[\"max_distance\"]==df_temp_all[\"dist\"]].copy()\n",
    "    df_temp = df_temp.drop(['x_0','y_0','z_0','max_distance'], axis=1)\n",
    "    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                         'atom_index_1': 'atom_index_farthest',\n",
    "                                         'dist': 'distance_farthest',\n",
    "                                         'x_1': 'x_farthest',\n",
    "                                         'y_1': 'y_farthest',\n",
    "                                         'z_1': 'z_farthest'})\n",
    "    df_temp = df_temp.drop_duplicates(subset=['molecule_name', 'atom_index'])\n",
    "        \n",
    "    for atom_idx in [0,1]:\n",
    "        df = map_atom_info(df,df_temp, atom_idx)\n",
    "        df = df.rename(columns={'atom_index_farthest': f'atom_index_farthest_{atom_idx}',\n",
    "                                        'distance_farthest': f'distance_farthest_{atom_idx}',\n",
    "                                        'x_farthest': f'x_farthest_{atom_idx}',\n",
    "                                        'y_farthest': f'y_farthest_{atom_idx}',\n",
    "                                        'z_farthest': f'z_farthest_{atom_idx}'})\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_cos_features(df):\n",
    "    \n",
    "    df[\"distance_center0\"] = np.sqrt((df['x_0']-df['c_x'])**2 \\\n",
    "                                   + (df['y_0']-df['c_y'])**2 \\\n",
    "                                   + (df['z_0']-df['c_z'])**2)\n",
    "    df[\"distance_center1\"] = np.sqrt((df['x_1']-df['c_x'])**2 \\\n",
    "                                   + (df['y_1']-df['c_y'])**2 \\\n",
    "                                   + (df['z_1']-df['c_z'])**2)\n",
    "    \n",
    "    df['distance_c0'] = np.sqrt((df['x_0']-df['x_closest_0'])**2 + \\\n",
    "                                (df['y_0']-df['y_closest_0'])**2 + \\\n",
    "                                (df['z_0']-df['z_closest_0'])**2)\n",
    "    df['distance_c1'] = np.sqrt((df['x_1']-df['x_closest_1'])**2 + \\\n",
    "                                (df['y_1']-df['y_closest_1'])**2 + \\\n",
    "                                (df['z_1']-df['z_closest_1'])**2)\n",
    "    \n",
    "    df[\"distance_f0\"] = np.sqrt((df['x_0']-df['x_farthest_0'])**2 + \\\n",
    "                                (df['y_0']-df['y_farthest_0'])**2 + \\\n",
    "                                (df['z_0']-df['z_farthest_0'])**2)\n",
    "    df[\"distance_f1\"] = np.sqrt((df['x_1']-df['x_farthest_1'])**2 + \\\n",
    "                                (df['y_1']-df['y_farthest_1'])**2 + \\\n",
    "                                (df['z_1']-df['z_farthest_1'])**2)\n",
    "    \n",
    "    vec_center0_x = (df['x_0']-df['c_x'])/(df[\"distance_center0\"]+1e-10)\n",
    "    vec_center0_y = (df['y_0']-df['c_y'])/(df[\"distance_center0\"]+1e-10)\n",
    "    vec_center0_z = (df['z_0']-df['c_z'])/(df[\"distance_center0\"]+1e-10)\n",
    "    \n",
    "    vec_center1_x = (df['x_1']-df['c_x'])/(df[\"distance_center1\"]+1e-10)\n",
    "    vec_center1_y = (df['y_1']-df['c_y'])/(df[\"distance_center1\"]+1e-10)\n",
    "    vec_center1_z = (df['z_1']-df['c_z'])/(df[\"distance_center1\"]+1e-10)\n",
    "    \n",
    "    vec_c0_x = (df['x_0']-df['x_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    vec_c0_y = (df['y_0']-df['y_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    vec_c0_z = (df['z_0']-df['z_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    \n",
    "    vec_c1_x = (df['x_1']-df['x_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    vec_c1_y = (df['y_1']-df['y_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    vec_c1_z = (df['z_1']-df['z_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    \n",
    "    vec_f0_x = (df['x_0']-df['x_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    vec_f0_y = (df['y_0']-df['y_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    vec_f0_z = (df['z_0']-df['z_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    \n",
    "    vec_f1_x = (df['x_1']-df['x_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    vec_f1_y = (df['y_1']-df['y_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    vec_f1_z = (df['z_1']-df['z_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    \n",
    "    vec_x = (df['x_1']-df['x_0'])/df['dist']\n",
    "    vec_y = (df['y_1']-df['y_0'])/df['dist']\n",
    "    vec_z = (df['z_1']-df['z_0'])/df['dist']\n",
    "    \n",
    "    df[\"cos_c0_c1\"] = vec_c0_x*vec_c1_x + vec_c0_y*vec_c1_y + vec_c0_z*vec_c1_z\n",
    "    df[\"cos_f0_f1\"] = vec_f0_x*vec_f1_x + vec_f0_y*vec_f1_y + vec_f0_z*vec_f1_z\n",
    "    \n",
    "    df[\"cos_c0_f0\"] = vec_c0_x*vec_f0_x + vec_c0_y*vec_f0_y + vec_c0_z*vec_f0_z\n",
    "    df[\"cos_c1_f1\"] = vec_c1_x*vec_f1_x + vec_c1_y*vec_f1_y + vec_c1_z*vec_f1_z\n",
    "    \n",
    "    df[\"cos_center0_center1\"] = vec_center0_x*vec_center1_x \\\n",
    "                              + vec_center0_y*vec_center1_y \\\n",
    "                              + vec_center0_z*vec_center1_z\n",
    "    \n",
    "    df[\"cos_c0\"] = vec_c0_x*vec_x + vec_c0_y*vec_y + vec_c0_z*vec_z\n",
    "    df[\"cos_c1\"] = vec_c1_x*vec_x + vec_c1_y*vec_y + vec_c1_z*vec_z\n",
    "    \n",
    "    df[\"cos_f0\"] = vec_f0_x*vec_x + vec_f0_y*vec_y + vec_f0_z*vec_z\n",
    "    df[\"cos_f1\"] = vec_f1_x*vec_x + vec_f1_y*vec_y + vec_f1_z*vec_z\n",
    "    \n",
    "    df[\"cos_center0\"] = vec_center0_x*vec_x + vec_center0_y*vec_y + vec_center0_z*vec_z\n",
    "    df[\"cos_center1\"] = vec_center1_x*vec_x + vec_center1_y*vec_y + vec_center1_z*vec_z\n",
    "\n",
    "    return df\n",
    "\n",
    "def dummies(df, list_cols):\n",
    "    for col in list_cols:\n",
    "        df_dummies = pd.get_dummies(df[col], drop_first=True, \n",
    "                                    prefix=(str(col)))\n",
    "        df = pd.concat([df, df_dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_qm9_features(df):\n",
    "    data_qm9 = pd.read_pickle('../input/quantum-machine-9-qm9/data.covs.pickle')\n",
    "    to_drop = ['type', \n",
    "               'linear', \n",
    "               'atom_index_0', \n",
    "               'atom_index_1', \n",
    "               'scalar_coupling_constant', \n",
    "               'U', 'G', 'H', \n",
    "               'mulliken_mean', 'r2', 'U0']\n",
    "    data_qm9 = data_qm9.drop(columns = to_drop, axis=1)\n",
    "    data_qm9 = reduce_mem_usage(data_qm9,verbose=False)\n",
    "    df = pd.merge(df, data_qm9, how='left', on=['molecule_name','id'])\n",
    "    del data_qm9\n",
    "    \n",
    "    df = dummies(df, ['type', 'atom_1'])\n",
    "    return df\n",
    "\n",
    "def get_features(df, struct):\n",
    "    for atom_idx in [0,1]:\n",
    "        df = map_atom_info(df, struct, atom_idx)\n",
    "        df = df.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                            'x': f'x_{atom_idx}',\n",
    "                            'y': f'y_{atom_idx}',\n",
    "                            'z': f'z_{atom_idx}'})\n",
    "        struct['c_x'] = struct.groupby('molecule_name')['x'].transform('mean')\n",
    "        struct['c_y'] = struct.groupby('molecule_name')['y'].transform('mean')\n",
    "        struct['c_z'] = struct.groupby('molecule_name')['z'].transform('mean')\n",
    "\n",
    "    df = find_dist(df)\n",
    "    df = find_closest_atom(df)\n",
    "    df = add_cos_features(df)\n",
    "    df = add_qm9_features(df)\n",
    "    return df\n",
    "\n",
    "def comp_score (y_true, y_pred, jtype):\n",
    "    df = pd.DataFrame()\n",
    "    df['y_true'] , df['y_pred'], df['jtype'] = y_true , y_pred, jtype\n",
    "    score = 0 \n",
    "    for t in jtype.unique():\n",
    "        score_jtype = np.log(mean_absolute_error(df[df.jtype==t]['y_true'],df[df.jtype==t]['y_pred']))\n",
    "        score += score_jtype\n",
    "        print(f'{t} : {score_jtype}')\n",
    "    score /= len(jtype.unique())\n",
    "    return score\n",
    "\n",
    "def feat_from_structures(df, st):\n",
    "    df = pd.merge(df,st,how='left',left_on=['molecule_name','atom_index_0'], right_on=['molecule_name','atom_index'])\n",
    "    df = pd.merge(df,st,how='left',left_on=['molecule_name','atom_index_1'], right_on=['molecule_name','atom_index'])\n",
    "    n_atoms = st.groupby(['molecule_name','atom'])['atom'].size().to_frame(name = 'count').reset_index()\n",
    "    n_atoms_df = n_atoms.pivot_table('count',['molecule_name'], 'atom')\n",
    "    n_atoms_df.fillna(0,inplace=True)\n",
    "    df = pd.merge(df,n_atoms_df,on=['molecule_name'],how='left')\n",
    "    del n_atoms\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "atomic_radius = {'H': 0.43, 'C': 0.82, 'N': 0.8, 'O': 0.78, 'F': 0.76}\n",
    "electronegativity = {'H': 2.2, 'C': 2.55, 'N': 3.04, 'O': 3.44, 'F': 3.98}\n",
    "\n",
    "\n",
    "struct = pd.read_csv('../input/structures.csv')\n",
    "pipeline_model1 = make_pipeline(MoreStructureProperties(atomic_radius,electronegativity))\n",
    "pipeline_model2 = make_pipeline(MakeMoreFeatures())\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "struct = pd.read_csv('../input/structures.csv')\n",
    "structures_yukawa = pd.read_csv('../input/structures_yukawa.csv')\n",
    "struct = pd.concat([struct, structures_yukawa], axis=1)\n",
    "del structures_yukawa\n",
    "struct = reduce_mem_usage(struct,verbose=False)\n",
    "gc.collect()\n",
    "train = get_features(train, struct.copy())\n",
    "test = get_features(test, struct.copy())\n",
    "y = train['scalar_coupling_constant']\n",
    "del struct\n",
    "gc.collect()\n",
    "\n",
    "struct = pd.read_csv('../input/structures.csv')\n",
    "struct = pipeline_model1.fit_transform(struct)\n",
    "train = feat_from_structures(train,struct)\n",
    "train = pipeline_model2.fit_transform(train.drop(['scalar_coupling_constant'],axis=1), train['scalar_coupling_constant'])\n",
    "test = feat_from_structures(test,struct)\n",
    "test = pipeline_model2.transform(test)\n",
    "train = reduce_mem_usage(train,verbose=False)\n",
    "test = reduce_mem_usage(test,verbose=False)\n",
    "\n",
    "giba_columns = ['inv_dist0', 'inv_dist1', 'inv_distP', 'inv_dist0R', 'inv_dist1R', 'inv_distPR', 'inv_dist0E', 'inv_dist1E', 'inv_distPE', 'linkM0',\n",
    "         'linkM1', 'min_molecule_atom_0_dist_xyz', 'mean_molecule_atom_0_dist_xyz', 'max_molecule_atom_0_dist_xyz', 'sd_molecule_atom_0_dist_xyz', 'min_molecule_atom_1_dist_xyz',\n",
    "         'mean_molecule_atom_1_dist_xyz', 'max_molecule_atom_1_dist_xyz', 'sd_molecule_atom_1_dist_xyz', 'coulomb_C.x', 'coulomb_F.x', 'coulomb_H.x', 'coulomb_N.x',\n",
    "         'coulomb_O.x', 'yukawa_C.x', 'yukawa_F.x', 'yukawa_H.x', 'yukawa_N.x', 'yukawa_O.x', 'vander_C.x', 'vander_F.x', 'vander_H.x', 'vander_N.x', 'vander_O.x',\n",
    "         'coulomb_C.y', 'coulomb_F.y', 'coulomb_H.y', 'coulomb_N.y', 'coulomb_O.y', 'yukawa_C.y', 'yukawa_F.y', 'yukawa_H.y', 'yukawa_N.y', 'yukawa_O.y', 'vander_C.y',\n",
    "         'vander_F.y', 'vander_H.y', 'vander_N.y', 'vander_O.y', 'distC0', 'distH0', 'distN0', 'distC1', 'distH1', 'distN1', 'adH1', 'adH2', 'adH3', 'adH4', 'adC1',\n",
    "         'adC2', 'adC3', 'adC4', 'adN1', 'adN2', 'adN3', 'adN4', 'NC', 'NH', 'NN', 'NF', 'NO']\n",
    "\n",
    "train_giba_t = pd.read_csv('../input/giba-molecular-features/train_giba.csv/train_giba.csv',\n",
    "                        header=0,  usecols=giba_columns)\n",
    "test_giba_t = pd.read_csv('../input/giba-molecular-features/test_giba.csv/test_giba.csv',\n",
    "                       header=0,  usecols=giba_columns)\n",
    "train_giba_t = reduce_mem_usage(train_giba_t, verbose=False)\n",
    "test_giba_t = reduce_mem_usage(test_giba_t, verbose=False)\n",
    "\n",
    "train = pd.concat((train,train_giba_t),axis=1)\n",
    "test = pd.concat((test,test_giba_t),axis=1)\n",
    "\n",
    "all_features = ['type',   'x_x', 'y_x','z_x', 'atom_y', 'x_y', 'y_y',\n",
    "       'z_y', 'n_bonds_y', 'C', 'F', 'H', 'N', 'O', 'distance', 'dist_mean_x','dist_mean_y',\n",
    "        'x_dist_abs', 'y_dist_abs', 'z_dist_abs','inv_distance3',\n",
    "       'molecule_atom_1_dist_std_diff','molecule_dist_mean_x',\n",
    "       'molecule_dist_mean_y','molecule_dist_std_x','molecule_dist_std_y','molecule_atom_0_dist_mean',\n",
    "       'molecule_atom_1_dist_mean','dist_mean_bond_y',\n",
    "       'n_no_bonds_x','n_no_bonds_y', 'dist_std_x', 'dist_std_y','dist_min_x','dist_min_y','dist_max_x', 'dist_max_y',\n",
    "       'molecule_dist_range_x','molecule_dist_range_y', 'dimension_x', 'dimension_y','dimension_z','molecule_dist_mean_bond_x',\n",
    "       'molecule_dist_mean_bond_x','dist_mean_no_bond_x','dist_mean_no_bond_y',\n",
    "       'dist_std_bond_y','dist_bond_min_y','dist_bond_max_y',\n",
    "       'range_dist_bond_y','dist_std_no_bond_x','dist_std_no_bond_y', 'dist_no_bond_min_x','dist_no_bond_min_y','dist_no_bond_max_x',\n",
    "       'dist_no_bond_max_y', 'range_dist_no_bond_x','range_dist_no_bond_y','dist_median_bond_y','dist_median_x',\n",
    "       'dist_median_y','dist_median_no_bond_x','dist_median_no_bond_y','molecule_type_dist_min','molecule_type_dist_max',\n",
    "       'molecule_dist_mean_no_bond_x','molecule_dist_mean_no_bond_y', 'n_diff_y','molecule_atom_index_0_dist_min_div','molecule_atom_index_0_dist_std_div',\n",
    "        'molecule_atom_index_0_dist_mean','molecule_atom_index_0_dist_max','molecule_atom_index_1_dist_mean','molecule_atom_index_1_dist_max',\n",
    "       'molecule_atom_index_1_dist_min','molecule_atom_index_1_dist_min_div','molecule_atom_index_1_dist_std_diff','molecule_atom_index_0_dist_mean_diff',\n",
    "        'molecule_atom_index_1_dist_mean_div','molecule_atom_index_1_dist_min_diff', 'rc_A', 'rc_B', 'rc_C', 'mu', 'alpha', 'homo', 'lumo', 'gap', 'zpve', 'Cv',\n",
    "         'freqs_min', 'freqs_max', 'freqs_mean', 'mulliken_min', 'mulliken_max', 'mulliken_atom_0', 'mulliken_atom_1',\n",
    "         'dist_C_0_x', 'dist_C_1_x', 'dist_C_2_x', 'dist_C_3_x', 'dist_C_4_x', 'dist_F_0_x', 'dist_F_1_x', 'dist_F_2_x', 'dist_H_0_x',\n",
    "         'dist_H_1_x', 'dist_H_2_x', 'dist_H_3_x', 'dist_H_4_x', 'dist_N_0_x', 'dist_N_1_x', 'dist_N_2_x', 'dist_N_3_x', 'dist_N_4_x', 'dist_O_0_x', 'dist_O_1_x',\n",
    "         'dist_O_2_x', 'dist_O_3_x', 'dist_O_4_x', 'dist_C_0_y', 'dist_C_1_y', 'dist_C_2_y', 'dist_C_3_y', 'dist_C_4_y', 'dist_F_0_y', 'dist_F_1_y', 'dist_F_2_y',\n",
    "         'dist_F_3_y', 'dist_F_4_y', 'dist_H_0_y', 'dist_H_1_y', 'dist_H_2_y', 'dist_H_3_y', 'dist_H_4_y', 'dist_N_0_y', 'dist_N_1_y', 'dist_N_2_y', 'dist_N_3_y',\n",
    "         'dist_N_4_y', 'dist_O_0_y', 'dist_O_1_y', 'dist_O_2_y', 'dist_O_3_y', 'dist_O_4_y','distance_closest_0', 'distance_closest_1', 'distance_farthest_0',\n",
    "         'distance_farthest_1','cos_c0_c1', 'cos_f0_f1','cos_c0_f0', 'cos_c1_f1', 'cos_center0_center1', 'cos_c0', 'cos_c1', 'cos_f0', 'cos_f1',\n",
    "         'cos_center0', 'cos_center1'] + giba_columns\n",
    "\n",
    "cat_features = ['atom_y']\n",
    "\n",
    "X = train[all_features]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2 , random_state=182)\n",
    "pred = np.zeros(X_val.shape[0])\n",
    "\n",
    "params = {'num_leaves': 50,\n",
    "          'min_child_samples': 79,\n",
    "          'min_data_in_leaf': 100,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.2,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1,\n",
    "          'reg_lambda': 0.3,\n",
    "          'colsample_bytree': 1.0,\n",
    "          'num_iterations': 4000\n",
    "         }\n",
    "d_map_val = dict(zip(list(y_val.index.values),list(np.arange(y_val.shape[0]))))\n",
    "\n",
    "\n",
    "X_test = test[all_features]\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['type'] = test['type']\n",
    "pred_sub = np.zeros(sub.shape[0])\n",
    "all_features.pop(0)\n",
    "unique_types = train['type'].unique()\n",
    "del train\n",
    "del test\n",
    "del struct\n",
    "del X\n",
    "del y\n",
    "del train_giba_t\n",
    "del test_giba_t\n",
    "gc.collect()\n",
    "\n",
    "gc.collect()\n",
    "rf_cols = ['rf_00','rf_01','rf_02','rf_03','rf04','rf05']\n",
    "rf_cols1 = ['rf_10','rf_11','rf12']\n",
    "for t in unique_types:\n",
    "    evals_result = {}\n",
    "    idx_train = X_train[X_train.type==t].index.values\n",
    "    idx_val = X_val[X_val.type==t].index.values\n",
    "    idx_sub = sub[sub.type==t].index.values\n",
    "    clf = ClassifierTransformer(RandomForestClassifier(),n_classes=5,cv=5)\n",
    "    clf1 = ClassifierTransformer(RandomForestClassifier(),n_classes=2,cv=5)\n",
    "    X_extra = np.hstack([clf.fit_transform(X_train[X_train.type==t].drop(['type'],axis=1), y_train[idx_train]),clf1.fit_transform(X_train[X_train.type==t].drop(['type'],axis=1), y_train[idx_train])])\n",
    "    X_extra_val = np.hstack([clf.transform(X_val[X_val.type==t].drop(['type'],axis=1)),clf1.transform(X_val[X_val.type==t].drop(['type'],axis=1))])\n",
    "    X_extra_test = np.hstack([clf.transform(X_test[X_test.type==t].drop(['type'],axis=1)),clf1.transform(X_test[X_test.type==t].drop(['type'],axis=1))])\n",
    "    X_p = pd.DataFrame(data=np.hstack([X_train[X_train.type==t].drop(['type'],axis=1).values,X_extra]), columns=list(X_train.drop(['type'],axis=1).columns)+rf_cols+rf_cols1)\n",
    "    X_p_val = pd.DataFrame(data=np.hstack([X_val[X_val.type==t].drop(['type'],axis=1).values,X_extra_val]), columns=list(X_val.drop(['type'],axis=1).columns)+rf_cols+rf_cols1)\n",
    "    X_p_test = pd.DataFrame(data=np.hstack([X_test[X_test.type==t].drop(['type'],axis=1).values,X_extra_test]), columns=list(X_test.drop(['type'],axis=1).columns)+rf_cols+rf_cols1)\n",
    "    gbm = lgb.LGBMRegressor()\n",
    "    gbm.fit(X_p, y_train[idx_train])\n",
    "    gbm.booster_.feature_importance()\n",
    "    \n",
    "    fea_imp_ = pd.DataFrame({'cols':X_p.columns, 'fea_imp':gbm.feature_importances_})\n",
    "    if t=='1JHC':\n",
    "        nb_feat=100\n",
    "    else:\n",
    "        nb_feat=90\n",
    "    remain_features = list(fea_imp_.loc[fea_imp_.fea_imp > 0].sort_values(by=['fea_imp'], ascending = False)['cols'].values[:nb_feat])\n",
    "    \n",
    "    \n",
    "    print(f'Training for {t}')\n",
    "    lgb_train = lgb.Dataset(X_p[remain_features],label=y_train[idx_train])\n",
    "    lgb_val = lgb.Dataset(X_p_val[remain_features],label=y_val[idx_val])\n",
    "    model = lgb.train(params=params, train_set=lgb_train,  valid_sets=[lgb_train,lgb_val], valid_names=['train','val'],\n",
    "                  fobj=None, feval=None, init_model=None, feature_name='auto', categorical_feature='auto',evals_result=evals_result,\n",
    "                  early_stopping_rounds=50,  verbose_eval=500)\n",
    "    val_map = [d_map_val[k] for k in list(idx_val)]\n",
    "    pred[val_map] = model.predict(X_p_val[remain_features])\n",
    "    pred_sub[idx_sub] = model.predict(X_p_test[remain_features])\n",
    "    del X_p\n",
    "    del X_p_val\n",
    "    del X_p_test\n",
    "    del X_extra\n",
    "    del X_extra_val\n",
    "    del X_extra_test\n",
    "    del clf\n",
    "    del clf1\n",
    "    del gbm\n",
    "    gc.collect()\n",
    "\n",
    "    ax = lgb.plot_metric(evals_result, metric='l1')\n",
    "    plt.title(f'{t}_mae')\n",
    "    plt.show()\n",
    "    plt.savefig(f'{t}_mae.png')\n",
    "    plt.clf()\n",
    "    ax = lgb.plot_importance(model, max_num_features=40)\n",
    "    plt.title(f'{t}_feature_importance')\n",
    "    plt.show()\n",
    "    plt.savefig(f'{t}_feature_importance.png')\n",
    "    plt.clf()\n",
    "  \n",
    "    sns.scatterplot(x=pred[val_map],y=y_val[idx_val]-pred[val_map])\n",
    "    plt.xlabel('target')\n",
    "    plt.ylabel('residual')\n",
    "    plt.title(f'{t}_residuals_plot')\n",
    "    plt.show()\n",
    "    plt.savefig(f'{t}_residuals_plot.png')\n",
    "    plt.clf()\n",
    "jtype = X_val['type']\n",
    "print('Competition Validation Score: ',comp_score(y_val,pred,jtype))\n",
    "\n",
    "print(f'Time to train: {time()-t0}')    \n",
    "    \n",
    "sub['scalar_coupling_constant'] = pred_sub\n",
    "sub[['id','scalar_coupling_constant']].to_csv('sub_lgb_model_individual.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
